{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.signal\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Tuple\n",
    "import gym\n",
    "from PIL import Image\n",
    "# from pyvirtualdisplay import Display\n",
    "# Display().start()\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.optim import RMSprop\n",
    "import gym\n",
    "import time\n",
    "from collections import namedtuple, deque\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import robosuite as suite\n",
    "from robosuite.controllers import load_controller_config\n",
    "from robosuite.controllers.controller_factory import reset_controllers\n",
    "from robosuite.utils import observables\n",
    "from robosuite.utils.input_utils import *\n",
    "from robosuite.robots import Bimanual\n",
    "import imageio\n",
    "import numpy as np\n",
    "import robosuite.utils.macros as macros\n",
    "macros.IMAGE_CONVENTION = \"opencv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prakash/anaconda3/envs/tf14-gpu/lib/python3.8/site-packages/neptune/internal/backends/hosted_client.py:50: NeptuneDeprecationWarning: The 'neptune-client' package has been deprecated and will be removed in the future. Install the 'neptune' package instead. For more, see https://docs.neptune.ai/setup/upgrading/\n",
      "  from neptune.version import version as neptune_client_version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEPTUNE_PROJECT\n",
      "https://app.neptune.ai/prakashmallick.000/DDPG-robosuite/\n"
     ]
    }
   ],
   "source": [
    "#nep_log = neptune.init_model(\n",
    "#    project=\"prak_exp/DDPG-robosuite\")\n",
    "import neptune\n",
    "neptune_log =neptune.init_project(project='DDPG-robosuite', api_token='eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI1Mjg2NjUxNy01ODNiLTQ3YjctYTQ2NS04MTA0NWQxNGM1ODkifQ==' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEPTUNE_PROJECT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6978/67282582.py:3: NeptuneWarning: To avoid unintended consumption of logging hours during interactive sessions, the following monitoring options are disabled unless set to 'True' when initializing the run: 'capture_stdout', 'capture_stderr', and 'capture_hardware_metrics'.\n",
      "  run = neptune.init_run(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/prakashmallick.000/DDPG-robosuite/e/DDPGROB-10\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 13 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 13 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/prakashmallick.000/DDPG-robosuite/e/DDPGROB-10/metadata\n"
     ]
    }
   ],
   "source": [
    "import neptune\n",
    "\n",
    "run = neptune.init_run(\n",
    "    project=\"prakashmallick.000/DDPG-robosuite\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI1Mjg2NjUxNy01ODNiLTQ3YjctYTQ2NS04MTA0NWQxNGM1ODkifQ==\",\n",
    ")  # your credentials\n",
    "\n",
    "params = {\"learning_rate\": 0.001, \"optimizer\": \"Adam\"}\n",
    "run[\"parameters\"] = params\n",
    "\n",
    "for epoch in range(10):\n",
    "    run[\"train/loss\"].append(0.9 ** epoch)\n",
    "\n",
    "run[\"eval/f1_score\"] = 0.66\n",
    "\n",
    "run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Current sensor for observable Labviewer_image is invalid.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/CosMic_RAYs_X-main/X_Robot/robosuite/robosuite/utils/observables.py:377\u001b[0m, in \u001b[0;36mObservable._check_sensor_validity\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    376\u001b[0m _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodality\n\u001b[0;32m--> 377\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_shape \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sensor({}))\u001b[39m.\u001b[39mshape\n\u001b[1;32m    378\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_number \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_shape) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_shape[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/CosMic_RAYs_X-main/X_Robot/robosuite/robosuite/environments/robot_env.py:404\u001b[0m, in \u001b[0;36mRobotEnv._create_camera_sensors.<locals>.camera_rgb\u001b[0;34m(obs_cache)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[39m@sensor\u001b[39m(modality\u001b[39m=\u001b[39mmodality)\n\u001b[1;32m    403\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcamera_rgb\u001b[39m(obs_cache):\n\u001b[0;32m--> 404\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msim\u001b[39m.\u001b[39;49mrender(\n\u001b[1;32m    405\u001b[0m         camera_name\u001b[39m=\u001b[39;49mcam_name,\n\u001b[1;32m    406\u001b[0m         width\u001b[39m=\u001b[39;49mcam_w,\n\u001b[1;32m    407\u001b[0m         height\u001b[39m=\u001b[39;49mcam_h,\n\u001b[1;32m    408\u001b[0m         depth\u001b[39m=\u001b[39;49mcam_d,\n\u001b[1;32m    409\u001b[0m     )\n\u001b[1;32m    410\u001b[0m     \u001b[39mif\u001b[39;00m cam_d:\n",
      "File \u001b[0;32mmjsim.pyx:153\u001b[0m, in \u001b[0;36mmujoco_py.cymj.MjSim.render\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mwrappers.pxi:1071\u001b[0m, in \u001b[0;36mmujoco_py.cymj.PyMjModel.camera_name2id\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No \"camera\" with name Labviewer exists. Available \"camera\" names = ('frontview', 'birdview', 'agentview', 'sideview', 'robot0_robotview', 'robot0_eye_in_hand', 'robot0_D435').",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 35\u001b[0m\n\u001b[1;32m     16\u001b[0m env \u001b[39m=\u001b[39m suite\u001b[39m.\u001b[39mmake(\n\u001b[1;32m     17\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions,\n\u001b[1;32m     18\u001b[0m     has_renderer\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m \n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     25\u001b[0m test_env \u001b[39m=\u001b[39m suite\u001b[39m.\u001b[39mmake(\n\u001b[1;32m     26\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions,\n\u001b[1;32m     27\u001b[0m     has_renderer\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m \n\u001b[1;32m     32\u001b[0m )\n\u001b[0;32m---> 35\u001b[0m video_env \u001b[39m=\u001b[39m suite\u001b[39m.\u001b[39;49mmake(\n\u001b[1;32m     36\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions,\n\u001b[1;32m     37\u001b[0m     has_renderer\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     38\u001b[0m     has_offscreen_renderer\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     39\u001b[0m     ignore_done\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     40\u001b[0m     use_camera_obs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     41\u001b[0m     use_object_obs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, \n\u001b[1;32m     42\u001b[0m     camera_names\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mLabviewer\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     43\u001b[0m     camera_heights\u001b[39m=\u001b[39;49m\u001b[39m512\u001b[39;49m,\n\u001b[1;32m     44\u001b[0m     camera_widths\u001b[39m=\u001b[39;49m\u001b[39m512\u001b[39;49m,\n\u001b[1;32m     45\u001b[0m )\n\u001b[1;32m     47\u001b[0m frame \u001b[39m=\u001b[39m []\n\u001b[1;32m     48\u001b[0m device\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/CosMic_RAYs_X-main/X_Robot/robosuite/robosuite/environments/base.py:38\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(env_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mif\u001b[39;00m env_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m REGISTERED_ENVS:\n\u001b[1;32m     33\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\n\u001b[1;32m     34\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mEnvironment \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m not found. Make sure it is a registered environment among: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m     35\u001b[0m             env_name, \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(REGISTERED_ENVS)\n\u001b[1;32m     36\u001b[0m         )\n\u001b[1;32m     37\u001b[0m     )\n\u001b[0;32m---> 38\u001b[0m \u001b[39mreturn\u001b[39;00m REGISTERED_ENVS[env_name](\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/CosMic_RAYs_X-main/X_Robot/robosuite/robosuite/environments/manipulation/door.py:178\u001b[0m, in \u001b[0;36mDoor.__init__\u001b[0;34m(self, robots, env_configuration, controller_configs, gripper_types, initialization_noise, use_latch, use_camera_obs, use_object_obs, reward_scale, reward_shaping, placement_initializer, has_renderer, has_offscreen_renderer, render_camera, render_collision_mesh, render_visual_mesh, render_gpu_device_id, control_freq, horizon, ignore_done, hard_reset, camera_names, camera_heights, camera_widths, camera_depths, camera_segmentations, renderer, renderer_config)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[39m# object placement initializer\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplacement_initializer \u001b[39m=\u001b[39m placement_initializer\n\u001b[0;32m--> 178\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    179\u001b[0m     robots\u001b[39m=\u001b[39;49mrobots,\n\u001b[1;32m    180\u001b[0m     env_configuration\u001b[39m=\u001b[39;49menv_configuration,\n\u001b[1;32m    181\u001b[0m     controller_configs\u001b[39m=\u001b[39;49mcontroller_configs,\n\u001b[1;32m    182\u001b[0m     mount_types\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mdefault\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    183\u001b[0m     gripper_types\u001b[39m=\u001b[39;49mgripper_types,\n\u001b[1;32m    184\u001b[0m     initialization_noise\u001b[39m=\u001b[39;49minitialization_noise,\n\u001b[1;32m    185\u001b[0m     use_camera_obs\u001b[39m=\u001b[39;49muse_camera_obs,\n\u001b[1;32m    186\u001b[0m     has_renderer\u001b[39m=\u001b[39;49mhas_renderer,\n\u001b[1;32m    187\u001b[0m     has_offscreen_renderer\u001b[39m=\u001b[39;49mhas_offscreen_renderer,\n\u001b[1;32m    188\u001b[0m     render_camera\u001b[39m=\u001b[39;49mrender_camera,\n\u001b[1;32m    189\u001b[0m     render_collision_mesh\u001b[39m=\u001b[39;49mrender_collision_mesh,\n\u001b[1;32m    190\u001b[0m     render_visual_mesh\u001b[39m=\u001b[39;49mrender_visual_mesh,\n\u001b[1;32m    191\u001b[0m     render_gpu_device_id\u001b[39m=\u001b[39;49mrender_gpu_device_id,\n\u001b[1;32m    192\u001b[0m     control_freq\u001b[39m=\u001b[39;49mcontrol_freq,\n\u001b[1;32m    193\u001b[0m     horizon\u001b[39m=\u001b[39;49mhorizon,\n\u001b[1;32m    194\u001b[0m     ignore_done\u001b[39m=\u001b[39;49mignore_done,\n\u001b[1;32m    195\u001b[0m     hard_reset\u001b[39m=\u001b[39;49mhard_reset,\n\u001b[1;32m    196\u001b[0m     camera_names\u001b[39m=\u001b[39;49mcamera_names,\n\u001b[1;32m    197\u001b[0m     camera_heights\u001b[39m=\u001b[39;49mcamera_heights,\n\u001b[1;32m    198\u001b[0m     camera_widths\u001b[39m=\u001b[39;49mcamera_widths,\n\u001b[1;32m    199\u001b[0m     camera_depths\u001b[39m=\u001b[39;49mcamera_depths,\n\u001b[1;32m    200\u001b[0m     camera_segmentations\u001b[39m=\u001b[39;49mcamera_segmentations,\n\u001b[1;32m    201\u001b[0m     renderer\u001b[39m=\u001b[39;49mrenderer,\n\u001b[1;32m    202\u001b[0m     renderer_config\u001b[39m=\u001b[39;49mrenderer_config,\n\u001b[1;32m    203\u001b[0m )\n",
      "File \u001b[0;32m~/CosMic_RAYs_X-main/X_Robot/robosuite/robosuite/environments/manipulation/manipulation_env.py:162\u001b[0m, in \u001b[0;36mManipulationEnv.__init__\u001b[0;34m(self, robots, env_configuration, controller_configs, mount_types, gripper_types, initialization_noise, use_camera_obs, has_renderer, has_offscreen_renderer, render_camera, render_collision_mesh, render_visual_mesh, render_gpu_device_id, control_freq, horizon, ignore_done, hard_reset, camera_names, camera_heights, camera_widths, camera_depths, camera_segmentations, renderer, renderer_config)\u001b[0m\n\u001b[1;32m    154\u001b[0m robot_configs \u001b[39m=\u001b[39m [\n\u001b[1;32m    155\u001b[0m     {\n\u001b[1;32m    156\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mgripper_type\u001b[39m\u001b[39m\"\u001b[39m: gripper_types[idx],\n\u001b[1;32m    157\u001b[0m     }\n\u001b[1;32m    158\u001b[0m     \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_robots)\n\u001b[1;32m    159\u001b[0m ]\n\u001b[1;32m    161\u001b[0m \u001b[39m# Run superclass init\u001b[39;00m\n\u001b[0;32m--> 162\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    163\u001b[0m     robots\u001b[39m=\u001b[39;49mrobots,\n\u001b[1;32m    164\u001b[0m     env_configuration\u001b[39m=\u001b[39;49menv_configuration,\n\u001b[1;32m    165\u001b[0m     controller_configs\u001b[39m=\u001b[39;49mcontroller_configs,\n\u001b[1;32m    166\u001b[0m     mount_types\u001b[39m=\u001b[39;49mmount_types,\n\u001b[1;32m    167\u001b[0m     initialization_noise\u001b[39m=\u001b[39;49minitialization_noise,\n\u001b[1;32m    168\u001b[0m     use_camera_obs\u001b[39m=\u001b[39;49muse_camera_obs,\n\u001b[1;32m    169\u001b[0m     has_renderer\u001b[39m=\u001b[39;49mhas_renderer,\n\u001b[1;32m    170\u001b[0m     has_offscreen_renderer\u001b[39m=\u001b[39;49mhas_offscreen_renderer,\n\u001b[1;32m    171\u001b[0m     render_camera\u001b[39m=\u001b[39;49mrender_camera,\n\u001b[1;32m    172\u001b[0m     render_collision_mesh\u001b[39m=\u001b[39;49mrender_collision_mesh,\n\u001b[1;32m    173\u001b[0m     render_visual_mesh\u001b[39m=\u001b[39;49mrender_visual_mesh,\n\u001b[1;32m    174\u001b[0m     render_gpu_device_id\u001b[39m=\u001b[39;49mrender_gpu_device_id,\n\u001b[1;32m    175\u001b[0m     control_freq\u001b[39m=\u001b[39;49mcontrol_freq,\n\u001b[1;32m    176\u001b[0m     horizon\u001b[39m=\u001b[39;49mhorizon,\n\u001b[1;32m    177\u001b[0m     ignore_done\u001b[39m=\u001b[39;49mignore_done,\n\u001b[1;32m    178\u001b[0m     hard_reset\u001b[39m=\u001b[39;49mhard_reset,\n\u001b[1;32m    179\u001b[0m     camera_names\u001b[39m=\u001b[39;49mcamera_names,\n\u001b[1;32m    180\u001b[0m     camera_heights\u001b[39m=\u001b[39;49mcamera_heights,\n\u001b[1;32m    181\u001b[0m     camera_widths\u001b[39m=\u001b[39;49mcamera_widths,\n\u001b[1;32m    182\u001b[0m     camera_depths\u001b[39m=\u001b[39;49mcamera_depths,\n\u001b[1;32m    183\u001b[0m     camera_segmentations\u001b[39m=\u001b[39;49mcamera_segmentations,\n\u001b[1;32m    184\u001b[0m     robot_configs\u001b[39m=\u001b[39;49mrobot_configs,\n\u001b[1;32m    185\u001b[0m     renderer\u001b[39m=\u001b[39;49mrenderer,\n\u001b[1;32m    186\u001b[0m     renderer_config\u001b[39m=\u001b[39;49mrenderer_config,\n\u001b[1;32m    187\u001b[0m )\n",
      "File \u001b[0;32m~/CosMic_RAYs_X-main/X_Robot/robosuite/robosuite/environments/robot_env.py:214\u001b[0m, in \u001b[0;36mRobotEnv.__init__\u001b[0;34m(self, robots, env_configuration, mount_types, controller_configs, initialization_noise, use_camera_obs, has_renderer, has_offscreen_renderer, render_camera, render_collision_mesh, render_visual_mesh, render_gpu_device_id, control_freq, horizon, ignore_done, hard_reset, camera_names, camera_heights, camera_widths, camera_depths, camera_segmentations, robot_configs, renderer, renderer_config)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrobot_configs \u001b[39m=\u001b[39m [\n\u001b[1;32m    201\u001b[0m     \u001b[39mdict\u001b[39m(\n\u001b[1;32m    202\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[39mfor\u001b[39;00m idx, robot_config \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(robot_configs)\n\u001b[1;32m    211\u001b[0m ]\n\u001b[1;32m    213\u001b[0m \u001b[39m# Run superclass init\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    215\u001b[0m     has_renderer\u001b[39m=\u001b[39;49mhas_renderer,\n\u001b[1;32m    216\u001b[0m     has_offscreen_renderer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhas_offscreen_renderer,\n\u001b[1;32m    217\u001b[0m     render_camera\u001b[39m=\u001b[39;49mrender_camera,\n\u001b[1;32m    218\u001b[0m     render_collision_mesh\u001b[39m=\u001b[39;49mrender_collision_mesh,\n\u001b[1;32m    219\u001b[0m     render_visual_mesh\u001b[39m=\u001b[39;49mrender_visual_mesh,\n\u001b[1;32m    220\u001b[0m     render_gpu_device_id\u001b[39m=\u001b[39;49mrender_gpu_device_id,\n\u001b[1;32m    221\u001b[0m     control_freq\u001b[39m=\u001b[39;49mcontrol_freq,\n\u001b[1;32m    222\u001b[0m     horizon\u001b[39m=\u001b[39;49mhorizon,\n\u001b[1;32m    223\u001b[0m     ignore_done\u001b[39m=\u001b[39;49mignore_done,\n\u001b[1;32m    224\u001b[0m     hard_reset\u001b[39m=\u001b[39;49mhard_reset,\n\u001b[1;32m    225\u001b[0m     renderer\u001b[39m=\u001b[39;49mrenderer,\n\u001b[1;32m    226\u001b[0m     renderer_config\u001b[39m=\u001b[39;49mrenderer_config,\n\u001b[1;32m    227\u001b[0m )\n",
      "File \u001b[0;32m~/CosMic_RAYs_X-main/X_Robot/robosuite/robosuite/environments/base.py:149\u001b[0m, in \u001b[0;36mMujocoEnv.__init__\u001b[0;34m(self, has_renderer, has_offscreen_renderer, render_camera, render_collision_mesh, render_visual_mesh, render_gpu_device_id, control_freq, horizon, ignore_done, hard_reset, renderer, renderer_config)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_observables \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mviewer\u001b[39m.\u001b[39m_setup_observables()\n\u001b[1;32m    148\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 149\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_observables \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setup_observables()\n\u001b[1;32m    151\u001b[0m \u001b[39m# check if viewer has get observations method and set a flag for future use.\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mviewer_get_obs \u001b[39m=\u001b[39m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mviewer, \u001b[39m\"\u001b[39m\u001b[39m_get_observations\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/CosMic_RAYs_X-main/X_Robot/robosuite/robosuite/environments/manipulation/door.py:337\u001b[0m, in \u001b[0;36mDoor._setup_observables\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_setup_observables\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    331\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[39m    Sets up observables to be used for this environment. Creates object-based observables if enabled\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \n\u001b[1;32m    334\u001b[0m \u001b[39m    Returns:\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \u001b[39m        OrderedDict: Dictionary mapping observable names to its corresponding Observable object\u001b[39;00m\n\u001b[1;32m    336\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     observables \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_setup_observables()\n\u001b[1;32m    339\u001b[0m     \u001b[39m# low-level object information\u001b[39;00m\n\u001b[1;32m    340\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_object_obs:\n\u001b[1;32m    341\u001b[0m         \u001b[39m# Get robot prefix and define observables modality\u001b[39;00m\n",
      "File \u001b[0;32m~/CosMic_RAYs_X-main/X_Robot/robosuite/robosuite/environments/robot_env.py:361\u001b[0m, in \u001b[0;36mRobotEnv._setup_observables\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[39m# Create observables for these cameras\u001b[39;00m\n\u001b[1;32m    360\u001b[0m     \u001b[39mfor\u001b[39;00m name, s \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(names, sensors):\n\u001b[0;32m--> 361\u001b[0m         observables[name] \u001b[39m=\u001b[39m Observable(\n\u001b[1;32m    362\u001b[0m             name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m    363\u001b[0m             sensor\u001b[39m=\u001b[39;49ms,\n\u001b[1;32m    364\u001b[0m             sampling_rate\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcontrol_freq,\n\u001b[1;32m    365\u001b[0m         )\n\u001b[1;32m    367\u001b[0m \u001b[39mreturn\u001b[39;00m observables\n",
      "File \u001b[0;32m~/CosMic_RAYs_X-main/X_Robot/robosuite/robosuite/utils/observables.py:206\u001b[0m, in \u001b[0;36mObservable.__init__\u001b[0;34m(self, name, sensor, corrupter, filter, delayer, sampling_rate, enabled, active)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_shape \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m,)  \u001b[39m# filled in during sensor check call\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[39m# Make sure sensor is working\u001b[39;00m\n\u001b[0;32m--> 206\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_sensor_validity()\n\u001b[1;32m    208\u001b[0m \u001b[39m# These values will be modified during update() call\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time_since_last_sample \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m  \u001b[39m# seconds\u001b[39;00m\n",
      "File \u001b[0;32m~/CosMic_RAYs_X-main/X_Robot/robosuite/robosuite/utils/observables.py:380\u001b[0m, in \u001b[0;36mObservable._check_sensor_validity\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_number \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_shape) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_shape[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    379\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m--> 380\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCurrent sensor for observable \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m is invalid.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname))\n",
      "\u001b[0;31mValueError\u001b[0m: Current sensor for observable Labviewer_image is invalid."
     ]
    }
   ],
   "source": [
    "'''options = {\n",
    "    'env_name': 'EElab_test2',\n",
    "    \"robots\": \"UR5e\"\n",
    "}\n",
    "controller_name = \"JOINT_VELOCITY\"\n",
    "options[\"controller_configs\"] = suite.load_controller_config(default_controller=controller_name)\n",
    "'''\n",
    "options = {\n",
    "    'env_name': 'Door',\n",
    "    \"robots\": \"UR5e\"\n",
    "}\n",
    "controller_name = \"JOINT_VELOCITY\"\n",
    "options[\"controller_configs\"] = suite.load_controller_config(default_controller=controller_name)\n",
    "\n",
    "\n",
    "env = suite.make(\n",
    "    **options,\n",
    "    has_renderer=False,\n",
    "    has_offscreen_renderer=True,\n",
    "    ignore_done=True,\n",
    "    use_camera_obs=False,\n",
    "\n",
    ")\n",
    "\n",
    "test_env = suite.make(\n",
    "    **options,\n",
    "    has_renderer=False,\n",
    "    has_offscreen_renderer=True,\n",
    "    ignore_done=True,\n",
    "    use_camera_obs=False,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "video_env = suite.make(\n",
    "    **options,\n",
    "    has_renderer=False,\n",
    "    has_offscreen_renderer=True,\n",
    "    ignore_done=True,\n",
    "    use_camera_obs=True,\n",
    "    use_object_obs=True, \n",
    "    camera_names='Labviewer',\n",
    "    camera_heights=512,\n",
    "    camera_widths=512,\n",
    ")\n",
    "\n",
    "frame = []\n",
    "device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device = ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(sizes, activation, output_activation=nn.Identity):\n",
    "    layers = []\n",
    "    for j in range(len(sizes)-1):\n",
    "        act = activation if j < len(sizes)-2 else output_activation\n",
    "        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class MLPActor(nn.Module):\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation, act_limit):\n",
    "        super().__init__()\n",
    "        pi_sizes = [obs_dim] + list(hidden_sizes) + [act_dim]\n",
    "        self.pi = mlp(pi_sizes, activation, nn.Tanh)\n",
    "        self.act_limit = act_limit\n",
    "\n",
    "    def forward(self, obs):\n",
    "        # Return output from network scaled to action space limits.\n",
    "        return self.act_limit * self.pi(obs)\n",
    "\n",
    "class MLPQFunction(nn.Module):\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation):\n",
    "        super().__init__()\n",
    "        self.q = mlp([obs_dim + act_dim] + list(hidden_sizes) + [1], activation)\n",
    "\n",
    "    def forward(self, obs, act):\n",
    "        q = self.q(torch.cat([obs, act], dim=-1))\n",
    "        return torch.squeeze(q, -1) # Critical to ensure q has right shape.\n",
    "\n",
    "class MLPActorCritic(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_sizes=(256,256),\n",
    "                 activation=nn.ReLU, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
    "        super().__init__()\n",
    "\n",
    "        obs_dim = 47\n",
    "        act_dim = 7\n",
    "        act_limit = 1\n",
    "\n",
    "        # build policy and value functions\n",
    "        self.pi = MLPActor(obs_dim, act_dim, hidden_sizes, activation, act_limit).to(device)\n",
    "        self.q = MLPQFunction(obs_dim, act_dim, hidden_sizes, activation).to(device)\n",
    "\n",
    "    def act(self, obs):\n",
    "        with torch.no_grad():\n",
    "            return self.pi(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('obs', 'act', 'rew', 'next_obs', 'done'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params = {\n",
    "    \"dropout\": 0.2,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"hid\": 512,\n",
    "    \"l\": 3,\n",
    "    \"seed\": 0,\n",
    "    \"steps_per_epoch\": 3000,\n",
    "    \"steps_video\": 1000,\n",
    "    \"epochs\": 100,\n",
    "    \"replay_size\": int(1e7),\n",
    "    \"gamma\": 0.99,\n",
    "    \"polyak\": 0.995,\n",
    "    \"pi_lr\": 1e-4,\n",
    "    \"q_lr\": 1e-4,\n",
    "    \"batch_size\": 1000,\n",
    "    \"start_steps\": 10000, \n",
    "    \"update_after\": 2000,\n",
    "    \"update_every\": 100,\n",
    "    \"act_noise\": 0.01,\n",
    "    \"num_test_episodes\": 1,\n",
    "    \"max_ep_len\": 1000\n",
    "}\n",
    "\n",
    "ac_kwargs=dict(hidden_sizes=[params[\"hid\"]]*params[\"l\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs_dim =  47\n",
      "act_dim =  6\n",
      "act_limit =  1\n"
     ]
    }
   ],
   "source": [
    "nep_log = neptune_log\n",
    "nep_log[\"parameters\"] = params\n",
    "\n",
    "torch.manual_seed(params[\"seed\"])\n",
    "np.random.seed(params[\"seed\"])\n",
    "\n",
    "obs_dim = 47\n",
    "print('obs_dim = ', obs_dim)\n",
    "act_dim = 6\n",
    "print('act_dim = ', act_dim)\n",
    "# Action limit for clamping: critically, assumes all dimensions share the same bound!\n",
    "act_limit = 1\n",
    "print('act_limit = ', act_limit)\n",
    "# Create actor-critic module and target networks\n",
    "ac = MLPActorCritic(**ac_kwargs)\n",
    "ac_targ = deepcopy(ac)\n",
    "\n",
    "# Freeze target networks with respect to optimizers (only update via polyak averaging)\n",
    "for p in ac_targ.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "memory = ReplayMemory(params[\"replay_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up function for computing DDPG Q-loss\n",
    "def compute_loss_q(data):\n",
    "\n",
    "    o = torch.cat(data.obs).float()\n",
    "    a = torch.cat(data.act).float()\n",
    "    r = torch.cat(data.rew).float()\n",
    "    o2 =torch.cat(data.next_obs).float()\n",
    "    d = torch.cat(data.done).float()\n",
    "\n",
    "    q = ac.q(o,a)\n",
    "\n",
    "\n",
    "    # Bellman backup for Q function\n",
    "    with torch.no_grad():\n",
    "        q_pi_targ = ac_targ.q(o2, ac_targ.pi(o2))\n",
    "        backup = r + params[\"gamma\"] * (1 - d) * q_pi_targ\n",
    "\n",
    "    # MSE loss against Bellman backup\n",
    "    loss_q = ((q - backup)**2).mean()\n",
    "\n",
    "    return loss_q\n",
    "\n",
    "# Set up function for computing DDPG pi loss\n",
    "def compute_loss_pi(data):\n",
    "\n",
    "    o = torch.cat(data.obs).float()\n",
    "\n",
    "    q_pi = ac.q(o, ac.pi(o))\n",
    "\n",
    "    return -q_pi.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_optimizer = RMSprop(ac.pi.parameters(), lr=params[\"pi_lr\"])\n",
    "q_optimizer = RMSprop(ac.q.parameters(), lr=params[\"q_lr\"])\n",
    "\n",
    "def update(data):\n",
    "    # First run one gradient descent step for Q.\n",
    "\n",
    "\n",
    "    q_optimizer.zero_grad()\n",
    "    loss_q = compute_loss_q(data)\n",
    "\n",
    "    loss_q.backward()\n",
    "\n",
    "    q_optimizer.step()\n",
    "\n",
    "\n",
    "    # Freeze Q-network so you don't waste computational effort \n",
    "    # computing gradients for it during the policy learning step.\n",
    "    for p in ac.q.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # Next run one gradient descent step for pi.\n",
    "    pi_optimizer.zero_grad()\n",
    "    loss_pi = compute_loss_pi(data)\n",
    "    loss_pi.backward()\n",
    "    pi_optimizer.step()\n",
    "\n",
    "    # Unfreeze Q-network so you can optimize it at next DDPG step.\n",
    "    for p in ac.q.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "\n",
    "\n",
    "    # Finally, update target networks by polyak averaging.\n",
    "    with torch.no_grad():\n",
    "        for p, p_targ in zip(ac.parameters(), ac_targ.parameters()):\n",
    "            # NB: We use an in-place operations \"mul_\", \"add_\" to update target\n",
    "            # params, as opposed to \"mul\" and \"add\", which would make new tensors.\n",
    "            p_targ.data.mul_(params[\"polyak\"])\n",
    "            p_targ.data.add_((1 - params[\"polyak\"]) * p.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def get_action(o, noise_scale):\n",
    "    a = ac.act(torch.as_tensor(o, dtype=torch.float32))\n",
    "    a += noise_scale * torch.randn(act_dim).to(device)\n",
    "    return torch.clip(a, -act_limit, act_limit)\n",
    "\n",
    "def test_agent(epoch):\n",
    "    test_main = 0\n",
    "    for j in range(params[\"num_test_episodes\"]):\n",
    "        obs, d, test_ep_ret, test_ep_len = test_env.reset(), False, 0, 0\n",
    "        o = list(obs['robot0_proprio-state']) + list(obs['object-state'])\n",
    "        o = torch.tensor([o], dtype=torch.float32, device=device)\n",
    "        while not(d or (test_ep_len == params[\"max_ep_len\"])):\n",
    "            a_cpu = get_action(o, 0).cpu().data.numpy()\n",
    "            obs, r, d, _ = test_env.step(a_cpu[0])\n",
    "            o = list(obs['robot0_proprio-state']) + list(obs['object-state'])\n",
    "            o = torch.tensor([o], dtype=torch.float32, device=device)\n",
    "            test_ep_ret += r\n",
    "            test_ep_len += 1\n",
    "            test_ep_main = test_ep_ret/test_ep_len\n",
    "\n",
    "        test_main += test_ep_main\n",
    "        print('test_rew_main = ', float(test_main))\n",
    "        nep_log[\"test/reward\"].log(test_main)\n",
    "    \n",
    "def video_agent(epoch):\n",
    "    obs, d, test_ep_len = video_env.reset(), False, 0\n",
    "    o = list(obs['robot0_proprio-state']) + list(obs['object-state'])\n",
    "    o = torch.tensor([o], dtype=torch.float32, device=device)\n",
    "    now = datetime.now()\n",
    "    current_time = str(now.isoformat())\n",
    "    writer = imageio.get_writer(\n",
    "        \"prak_exp/DDPG-robosuite/DDPG_UR5_%s_ep_%d.mp4\" % (current_time, epoch), fps=100)\n",
    "    frame = obs[\"Labviewer_image\"]\n",
    "    writer.append_data(frame)\n",
    "\n",
    "    while not(d or (test_ep_len == params[\"max_ep_len\"])):\n",
    "        n = torch.zeros(act_dim)\n",
    "        a_cpu = get_action(o, n).cpu().data.numpy()\n",
    "        obs, _, d, _ = video_env.step(a_cpu[0])\n",
    "        o = list(obs['robot0_proprio-state']) + list(obs['object-state'])\n",
    "        o = torch.tensor([o], dtype=torch.float32, device=device)\n",
    "        frame = obs[\"Labviewer_image\"]\n",
    "        writer.append_data(frame)\n",
    "    writer.close()\n",
    "    nep_log['video'] = neptune.types.File('prak_exp/DDPG-robosuite/DDPG_UR5_%s_ep_%d.mp4' % (current_time, epoch))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obs = {\n",
    "#     'robot0_joint_pos_cos': None,\n",
    "#     'robot0_joint_pos_sin': None,\n",
    "#     'robot0_joint_vel': None,\n",
    "#     'robot0_eef_pos': None,\n",
    "#     'robot0_eef_quat': None,\n",
    "#     'robot0_gripper_qpos': None,\n",
    "#     'robot0_gripper_qvel': None,\n",
    "#     'cubeA_pos': None,\n",
    "#     'cubeA_quat': None,\n",
    "#     'cubeB_pos': None,\n",
    "#     'cubeB_quat': None,\n",
    "#     'gripper_to_cubeA': None,\n",
    "#     'gripper_to_cubeB': None,\n",
    "#     'cubeA_to_cubeB': None,\n",
    "# }\n",
    "\n",
    "obs, ep_ret, ep_len = env.reset(), 0, 0\n",
    "\n",
    "o = list(obs['robot0_proprio-state']) + list(obs['object-state'])\n",
    "\n",
    "# env.viewer.set_camera(camera_id=0)\n",
    "\n",
    "\n",
    "# Define neutral value\n",
    "neutral = np.zeros(7)\n",
    "\n",
    "# Keep track of done variable to know when to break loop\n",
    "\n",
    "# Prepare for interaction with environment\n",
    "total_steps = params[\"steps_per_epoch\"] * params[\"epochs\"]\n",
    "start_time = time.time()\n",
    "\n",
    "o = torch.tensor([o], device=device)\n",
    "\n",
    "\n",
    "start_time_rec = datetime.now()\n",
    "r_true = 0\n",
    "total_main = 0\n",
    "ep_rew_main = 0\n",
    "reward_dict={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1115/300000 [00:02<18:35, 268.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_time =  2023-05-03T21:20:42.191716\n",
      "current_time =  2023-05-03T21:20:42.191782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2000/300000 [00:04<12:02, 412.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_time =  2023-05-03T21:20:44.658428\n",
      "current_time =  2023-05-03T21:20:44.658513\n",
      " ** On entry to SGEMM  parameter number 10 had an illegal value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_INVALID_VALUE when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 57\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[39m# Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\u001b[39;00m\n\u001b[1;32m     54\u001b[0m         \u001b[39m# detailed explanation). This converts batch-array of Transitions\u001b[39;00m\n\u001b[1;32m     55\u001b[0m         \u001b[39m# to Transition of batch-arrays.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m         batch \u001b[39m=\u001b[39m Transition(\u001b[39m*\u001b[39m\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mtransitions))\n\u001b[0;32m---> 57\u001b[0m         update(data\u001b[39m=\u001b[39;49mbatch)\n\u001b[1;32m     59\u001b[0m \u001b[39m# End of epoch handling\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[39mif\u001b[39;00m (t\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m params[\u001b[39m\"\u001b[39m\u001b[39msteps_per_epoch\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[13], line 9\u001b[0m, in \u001b[0;36mupdate\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate\u001b[39m(data):\n\u001b[1;32m      5\u001b[0m     \u001b[39m# First run one gradient descent step for Q.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     q_optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m----> 9\u001b[0m     loss_q \u001b[39m=\u001b[39m compute_loss_q(data)\n\u001b[1;32m     11\u001b[0m     loss_q\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     13\u001b[0m     q_optimizer\u001b[39m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[12], line 10\u001b[0m, in \u001b[0;36mcompute_loss_q\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      7\u001b[0m o2 \u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mcat(data\u001b[39m.\u001b[39mnext_obs)\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m      8\u001b[0m d \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(data\u001b[39m.\u001b[39mdone)\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m---> 10\u001b[0m q \u001b[39m=\u001b[39m ac\u001b[39m.\u001b[39;49mq(o,a)\n\u001b[1;32m     13\u001b[0m \u001b[39m# Bellman backup for Q function\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/anaconda3/envs/tf14-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[8], line 28\u001b[0m, in \u001b[0;36mMLPQFunction.forward\u001b[0;34m(self, obs, act)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, obs, act):\n\u001b[0;32m---> 28\u001b[0m     q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mq(torch\u001b[39m.\u001b[39;49mcat([obs, act], dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[1;32m     29\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39msqueeze(q, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf14-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/tf14-gpu/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf14-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/tf14-gpu/lib/python3.8/site-packages/torch/nn/modules/linear.py:96\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf14-gpu/lib/python3.8/site-packages/torch/nn/functional.py:1847\u001b[0m, in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight):\n\u001b[1;32m   1846\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[39minput\u001b[39m, weight), \u001b[39minput\u001b[39m, weight, bias\u001b[39m=\u001b[39mbias)\n\u001b[0;32m-> 1847\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, weight, bias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_INVALID_VALUE when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`"
     ]
    }
   ],
   "source": [
    "# Main loop: collect experience in env and update/log each epoch\n",
    "low, high = env.action_spec\n",
    "\n",
    "for t in tqdm(range(total_steps)):\n",
    "    \n",
    "    # Until start_steps have elapsed, randomly sample actions\n",
    "    # from a uniform distribution for better exploration. Afterwards, \n",
    "    # use the learned policy (with some noise, via act_noise). \n",
    "    if t > params[\"start_steps\"]:\n",
    "        a = get_action(o, params[\"act_noise\"])      # Tensor\n",
    "    else:\n",
    "        a = torch.tensor([np.random.uniform(low, high)], dtype=torch.float32, device=device)\n",
    "        \n",
    "    a_cpu = a.cpu().data.numpy()\n",
    "    # Step the env\n",
    "    obs2, r, d, _ = env.step(a_cpu[0])\n",
    "    o2 = list(obs2['robot0_proprio-state']) + list(obs2['object-state'])\n",
    "\n",
    "    ep_len += 1\n",
    "    total_main += r\n",
    "\n",
    "\n",
    "    # Ignore the \"done\" signal if it comes from hitting the time\n",
    "    # horizon (that is, when it's an artificial terminal signal\n",
    "    # that isn't based on the agent's state)\n",
    "    d = False if ep_len==params[\"max_ep_len\"] else d\n",
    "\n",
    "    o2 = torch.tensor([o2], dtype=torch.float32, device=device)\n",
    "    r = torch.tensor([r], dtype=torch.float32, device=device)\n",
    "    d = torch.tensor([d], dtype=torch.float32, device=device)\n",
    "\n",
    "    # Store experience to replay buffer\n",
    "    memory.push(o, a, r, o2, d)\n",
    "    # Super critical, easy to overlook step: make sure to update \n",
    "    # most recent observation!\n",
    "    o=o2\n",
    "    ep_ret += r\n",
    "    \n",
    "    \n",
    "    # End of trajectory handling\n",
    "    if d or (ep_len == params[\"max_ep_len\"]):\n",
    "        sum_ep_rew = ep_ret\n",
    "        obs, ep_ret, ep_len = env.reset(), 0, 0\n",
    "        o = list(obs['robot0_proprio-state']) + list(obs['object-state'])\n",
    "        o = torch.tensor([o], device=device)\n",
    "\n",
    "\n",
    "    # Update handling\n",
    "    if t >= params[\"update_after\"] and t % params[\"update_every\"] == 0:\n",
    "        for i in range(params[\"update_every\"]):\n",
    "\n",
    "            transitions = memory.sample(params[\"batch_size\"])\n",
    "            # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "            # detailed explanation). This converts batch-array of Transitions\n",
    "            # to Transition of batch-arrays.\n",
    "            batch = Transition(*zip(*transitions))\n",
    "            update(data=batch)\n",
    "\n",
    "    # End of epoch handling\n",
    "    if (t+1) % params[\"steps_per_epoch\"] == 0:\n",
    "        epoch = (t+1) // params[\"steps_per_epoch\"]\n",
    "        ep_rew_main = sum_ep_rew/epoch\n",
    "\n",
    "        nep_log[\"train/reward\"].log(ep_rew_main)\n",
    "        print('ep_rew_main = ', ep_rew_main.cpu().data.numpy())\n",
    "        # Test the performance of the deterministic version of the agent.\n",
    "        test_agent(epoch)\n",
    "        \n",
    "\n",
    "    if (t+1) % params[\"steps_video\"] == 0:\n",
    "        epoch = (t+1) // params[\"steps_per_epoch\"]\n",
    "        now = datetime.now()\n",
    "        current_time = str(now.isoformat())\n",
    "        print('current_time = ', current_time)\n",
    "        #video_agent(epoch)\n",
    "        now = datetime.now()\n",
    "        current_time = str(now.isoformat())\n",
    "        print('current_time = ', current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act=(tensor([ 0.0056,  0.0088,  0.0041, -0.0196,  0.0318,  0.0571, -0.0213], device='cuda:0'), \n",
    "       tensor([ 0.0072,  0.0174,  0.0155, -0.0142,  0.0314,  0.0644, -0.0250], device='cuda:0'), \n",
    "       tensor([ 0.0105,  0.0098,  0.0013, -0.0352,  0.0362,  0.0726, -0.0136], device='cuda:0'), \n",
    "       tensor([[-0.0859,  0.9599, -0.1526,  0.7142, -0.7654, -0.4575, -0.1924]],device='cuda:0', dtype=torch.float64),\n",
    "       tensor([[ 0.9379,  0.8099, -0.4069,  0.9840, -0.5012, -0.7882,  0.9019]],device='cuda:0', dtype=torch.float64), \n",
    "       tensor([ 0.0222, -0.0113,  0.0070, -0.0278,  0.0469,  0.0983, -0.0294],device='cuda:0'), \n",
    "       tensor([[-0.6993,  0.1166, -0.1432,  0.8463, -0.7898,  0.9651,  0.7509]],device='cuda:0', dtype=torch.float64), \n",
    "       tensor([[ 0.8086, -0.1013, -0.6988,  0.1877,  0.0752, -0.6032,  0.2061]],device='cuda:0', dtype=torch.float64), \n",
    "       tensor([[-0.4980, -0.8692, -0.1736,  0.1549, -0.0070, -0.6744, -0.2569]],device='cuda:0', dtype=torch.float64),\n",
    "       tensor([[-0.7246,  0.2981,  0.7121, -0.8136,  0.5074,  0.3804,  0.7333]],device='cuda:0', dtype=torch.float64), \n",
    "       tensor([-0.0044,  0.0087,  0.0068,  0.0011,  0.0439,  0.0870, -0.0187],device='cuda:0'), \n",
    "       tensor([[ 0.8305, -0.6940,  0.9806, -0.8814, -0.8822,  0.6465,  0.5532]],device='cuda:0', dtype=torch.float64), \n",
    "       tensor([ 0.0135, -0.0154,  0.0162, -0.0319,  0.0514,  0.0762, -0.0252],device='cuda:0'), \n",
    "       tensor([ 0.0095,  0.0081,  0.0254, -0.0023,  0.0521,  0.0687, -0.0063], device='cuda:0'), \n",
    "       tensor([ 0.0149, -0.0034, -0.0068, -0.0371,  0.0315,  0.0860, -0.0123], device='cuda:0'), \n",
    "       tensor([ 0.0245, -0.0133,  0.0085, -0.0075,  0.0582,  0.0831, -0.0191],device='cuda:0'), \n",
    "       tensor([[ 0.1396, -0.0016,  0.2254,  0.4455,  0.3278,  0.5242, -0.7074]],device='cuda:0', dtype=torch.float64), \n",
    "       tensor([[-0.1422, -0.1287, -0.7492, -0.6580,  0.8380, -0.0847, -0.0551]],device='cuda:0', dtype=torch.float64), \n",
    "       tensor([-0.0010,  0.0225,  0.0193, -0.0460,  0.0465,  0.0715, -0.0418],device='cuda:0'), \n",
    "       tensor([ 0.0283, -0.0028,  0.0214, -0.0092,  0.0324,  0.0658, -0.0111],device='cuda:0'), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('tf14-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fa816d608fc7e1febdba4bb7b571b1789c06395f462d8f6b87bf155568e406c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
