{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.signal\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Tuple\n",
    "import gym\n",
    "from PIL import Image\n",
    "# from pyvirtualdisplay import Display\n",
    "# Display().start()\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.optim import RMSprop\n",
    "import gym\n",
    "import time\n",
    "from collections import namedtuple, deque\n",
    "import neptune.new as neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import robosuite as suite\n",
    "from robosuite.controllers import load_controller_config\n",
    "from robosuite.controllers.controller_factory import reset_controllers\n",
    "from robosuite.utils import observables\n",
    "from robosuite.utils.input_utils import *\n",
    "from robosuite.robots import Bimanual\n",
    "import imageio\n",
    "import numpy as np\n",
    "import robosuite.utils.macros as macros\n",
    "macros.IMAGE_CONVENTION = \"opencv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/xhnfirst/DDPG-robosuite/e/DDPGROB-150\n",
      "Remember to stop your run once youâ€™ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
     ]
    }
   ],
   "source": [
    "nep_log = neptune.init(\n",
    "    project=\"xhnfirst/DDPG-robosuite\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI1NTg5MDI2OS01MTVmLTQ2YjUtODA1Yy02ZWQyNDgxZDcwN2UifQ==\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device =  cuda\n"
     ]
    }
   ],
   "source": [
    "options = {\n",
    "    'env_name': 'EElab_test2',\n",
    "    \"robots\": \"UR5e\"\n",
    "}\n",
    "controller_name = \"JOINT_VELOCITY\"\n",
    "options[\"controller_configs\"] = suite.load_controller_config(default_controller=controller_name)\n",
    "\n",
    "env = suite.make(\n",
    "    **options,\n",
    "    has_renderer=False,\n",
    "    has_offscreen_renderer=True,\n",
    "    ignore_done=True,\n",
    "    use_camera_obs=False,\n",
    "    gripper_types=None,\n",
    "    renderer = 'mujoco',\n",
    "\n",
    ")\n",
    "\n",
    "test_env = suite.make(\n",
    "    **options,\n",
    "    has_renderer=False,\n",
    "    has_offscreen_renderer=True,\n",
    "    ignore_done=True,\n",
    "    use_camera_obs=False,\n",
    "    gripper_types=None,\n",
    "    renderer = 'mujoco',\n",
    ")\n",
    "\n",
    "\n",
    "video_env = suite.make(\n",
    "    **options,\n",
    "    gripper_types=None,\n",
    "    has_renderer=False,\n",
    "    has_offscreen_renderer=True,\n",
    "    ignore_done=True,\n",
    "    use_camera_obs=True,\n",
    "    use_object_obs=True, \n",
    "    camera_names='Labviewer',\n",
    "    camera_heights=512,\n",
    "    camera_widths=512,\n",
    "    # control_freq=200,\n",
    "    renderer = 'mujoco',\n",
    ")\n",
    "\n",
    "frame = []\n",
    "device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device = ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(sizes, activation, output_activation=nn.Identity):\n",
    "    layers = []\n",
    "    for j in range(len(sizes)-1):\n",
    "        act = activation if j < len(sizes)-2 else output_activation\n",
    "        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class MLPActor(nn.Module):\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation, act_limit):\n",
    "        super().__init__()\n",
    "        pi_sizes = [obs_dim] + list(hidden_sizes) + [act_dim]\n",
    "        self.pi = mlp(pi_sizes, activation, nn.Tanh)\n",
    "        self.act_limit = act_limit\n",
    "\n",
    "    def forward(self, obs):\n",
    "        # Return output from network scaled to action space limits.\n",
    "        return self.act_limit * self.pi(obs)\n",
    "\n",
    "class MLPQFunction(nn.Module):\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation):\n",
    "        super().__init__()\n",
    "        self.q = mlp([obs_dim + act_dim] + list(hidden_sizes) + [1], activation)\n",
    "\n",
    "    def forward(self, obs, act):\n",
    "        q = self.q(torch.cat([obs, act], dim=-1))\n",
    "        return torch.squeeze(q, -1) # Critical to ensure q has right shape.\n",
    "\n",
    "class MLPActorCritic(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_sizes=(256,256),\n",
    "                 activation=nn.ReLU, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
    "        super().__init__()\n",
    "\n",
    "        obs_dim = 35\n",
    "        act_dim = 6\n",
    "        act_limit = 1\n",
    "\n",
    "        # build policy and value functions\n",
    "        self.pi = MLPActor(obs_dim, act_dim, hidden_sizes, activation, act_limit).to(device)\n",
    "        self.q = MLPQFunction(obs_dim, act_dim, hidden_sizes, activation).to(device)\n",
    "\n",
    "    def act(self, obs):\n",
    "        with torch.no_grad():\n",
    "            return self.pi(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('obs', 'act', 'rew', 'next_obs', 'done'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params = {\n",
    "    \"dropout\": 0.2,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"hid\": 256,\n",
    "    \"l\": 3,\n",
    "    \"seed\": 0,\n",
    "    \"steps_per_epoch\": 3000,\n",
    "    \"steps_video\": 30000,\n",
    "    \"epochs\": 1000,\n",
    "    \"replay_size\": int(1e8),\n",
    "    \"gamma\": 0.99,\n",
    "    \"polyak\": 0.995,\n",
    "    \"pi_lr\": 1e-4,\n",
    "    \"q_lr\": 1e-4,\n",
    "    \"batch_size\": 1000,\n",
    "    \"start_steps\": 10000, \n",
    "    \"update_after\": 5000,\n",
    "    \"update_every\": 100,\n",
    "    \"act_noise\": 0.01,\n",
    "    \"num_test_episodes\": 5,\n",
    "    \"max_ep_len\": 1000,\n",
    "    \"max_video_len\": 1000,\n",
    "    \"save_model_len\": 10000,\n",
    "    # \"obs_dim\": 47,\n",
    "    # \"act_dim\": 7,\n",
    "    # \"act_limit\": 1\n",
    "}\n",
    "\n",
    "ac_kwargs=dict(hidden_sizes=[params[\"hid\"]]*params[\"l\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs_dim =  35\n",
      "act_dim =  6\n",
      "act_limit =  1\n"
     ]
    }
   ],
   "source": [
    "nep_log[\"parameters\"] = params\n",
    "\n",
    "torch.manual_seed(params[\"seed\"])\n",
    "np.random.seed(params[\"seed\"])\n",
    "\n",
    "obs_dim = 35\n",
    "print('obs_dim = ', obs_dim)\n",
    "act_dim = 6\n",
    "print('act_dim = ', act_dim)\n",
    "# Action limit for clamping: critically, assumes all dimensions share the same bound!\n",
    "act_limit = 1\n",
    "print('act_limit = ', act_limit)\n",
    "# Create actor-critic module and target networks\n",
    "ac = MLPActorCritic(**ac_kwargs)\n",
    "ac_targ = deepcopy(ac)\n",
    "\n",
    "# Freeze target networks with respect to optimizers (only update via polyak averaging)\n",
    "for p in ac_targ.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "memory = ReplayMemory(params[\"replay_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up function for computing DDPG Q-loss\n",
    "def compute_loss_q(data):\n",
    "\n",
    "    o = torch.cat(data.obs).float()\n",
    "    a = torch.cat(data.act).float()\n",
    "    r = torch.cat(data.rew).float()\n",
    "    o2 =torch.cat(data.next_obs).float()\n",
    "    d = torch.cat(data.done).float()\n",
    "\n",
    "    q = ac.q(o,a)\n",
    "\n",
    "\n",
    "    # Bellman backup for Q function\n",
    "    with torch.no_grad():\n",
    "        q_pi_targ = ac_targ.q(o2, ac_targ.pi(o2))\n",
    "        backup = r + params[\"gamma\"] * (1 - d) * q_pi_targ\n",
    "\n",
    "    # MSE loss against Bellman backup\n",
    "    loss_q = ((q - backup)**2).mean()\n",
    "\n",
    "    return loss_q\n",
    "\n",
    "# Set up function for computing DDPG pi loss\n",
    "def compute_loss_pi(data):\n",
    "\n",
    "    o = torch.cat(data.obs).float()\n",
    "\n",
    "    q_pi = ac.q(o, ac.pi(o))\n",
    "\n",
    "    return -q_pi.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_optimizer = RMSprop(ac.pi.parameters(), lr=params[\"pi_lr\"])\n",
    "q_optimizer = RMSprop(ac.q.parameters(), lr=params[\"q_lr\"])\n",
    "\n",
    "def update(data):\n",
    "    # First run one gradient descent step for Q.\n",
    "\n",
    "\n",
    "    q_optimizer.zero_grad()\n",
    "    loss_q = compute_loss_q(data)\n",
    "\n",
    "    loss_q.backward()\n",
    "\n",
    "    q_optimizer.step()\n",
    "\n",
    "\n",
    "    # Freeze Q-network so you don't waste computational effort \n",
    "    # computing gradients for it during the policy learning step.\n",
    "    for p in ac.q.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # Next run one gradient descent step for pi.\n",
    "    pi_optimizer.zero_grad()\n",
    "    loss_pi = compute_loss_pi(data)\n",
    "    loss_pi.backward()\n",
    "    pi_optimizer.step()\n",
    "\n",
    "    # Unfreeze Q-network so you can optimize it at next DDPG step.\n",
    "    for p in ac.q.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "\n",
    "\n",
    "    # Finally, update target networks by polyak averaging.\n",
    "    with torch.no_grad():\n",
    "        for p, p_targ in zip(ac.parameters(), ac_targ.parameters()):\n",
    "            # NB: We use an in-place operations \"mul_\", \"add_\" to update target\n",
    "            # params, as opposed to \"mul\" and \"add\", which would make new tensors.\n",
    "            p_targ.data.mul_(params[\"polyak\"])\n",
    "            p_targ.data.add_((1 - params[\"polyak\"]) * p.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def get_action(o, noise_scale):\n",
    "    a = ac.act(torch.as_tensor(o, dtype=torch.float32))\n",
    "    a += noise_scale * torch.randn(act_dim).to(device)\n",
    "    return torch.clip(a, -act_limit, act_limit)\n",
    "\n",
    "def test_agent(epoch):\n",
    "    test_main = 0\n",
    "    test_step = 0\n",
    "    for j in range(params[\"num_test_episodes\"]):\n",
    "        obs, d, test_ep_ret, test_ep_len = test_env.reset(), False, 0, 0\n",
    "        o = list(obs['robot0_proprio-state']) + list(obs['object-state'])\n",
    "        o = torch.tensor([o], dtype=torch.float32, device=device)\n",
    "        while not(d or (test_ep_len == params[\"max_ep_len\"])):\n",
    "            a_cpu = get_action(o, 0).cpu().data.numpy()\n",
    "            obs, r, d, _ = test_env.step(a_cpu[0])\n",
    "            o = list(obs['robot0_proprio-state']) + list(obs['object-state'])\n",
    "            o = torch.tensor([o], dtype=torch.float32, device=device)\n",
    "            test_ep_ret += r\n",
    "            test_ep_len += 1\n",
    "        test_ep_main = test_ep_ret/test_ep_len\n",
    "        test_step +=1\n",
    "        test_main += test_ep_main\n",
    "    print('test_rew_main = ', float(test_main/test_step))\n",
    "    nep_log[\"test/reward\"].log(test_main/test_step)\n",
    "    \n",
    "def video_agent(epoch):\n",
    "    obs, d, test_ep_len = video_env.reset(), False, 0\n",
    "    o = list(obs['robot0_proprio-state']) + list(obs['object-state'])\n",
    "    o = torch.tensor([o], dtype=torch.float32, device=device)\n",
    "    now = datetime.now()\n",
    "    current_time = str(now.isoformat())\n",
    "    writer = imageio.get_writer(\n",
    "        \"/home/xhnfly/Cosmic_rays_X/X_Robot/robosuite/robosuite/demos/video/DDPG_UR5_%s_ep_%d.mp4\" % (current_time, epoch), fps=100)\n",
    "    frame = obs[\"Labviewer_image\"]\n",
    "    writer.append_data(frame)\n",
    "\n",
    "    while not(d or (test_ep_len == params[\"max_video_len\"])):\n",
    "        a_cpu = get_action(o, 0).cpu().data.numpy()\n",
    "        obs, _, d, _ = video_env.step(a_cpu[0])\n",
    "        o = list(obs['robot0_proprio-state']) + list(obs['object-state'])\n",
    "        o = torch.tensor([o], dtype=torch.float32, device=device)\n",
    "        frame = obs[\"Labviewer_image\"]\n",
    "        writer.append_data(frame)\n",
    "        test_ep_len += 1\n",
    "    writer.close()\n",
    "    nep_log['video'] = neptune.types.File('/home/xhnfly/Cosmic_rays_X/X_Robot/robosuite/robosuite/demos/video/DDPG_UR5_%s_ep_%d.mp4' % (current_time, epoch))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obs = {\n",
    "#     'robot0_joint_pos_cos': None,\n",
    "#     'robot0_joint_pos_sin': None,\n",
    "#     'robot0_joint_vel': None,\n",
    "#     'robot0_eef_pos': None,\n",
    "#     'robot0_eef_quat': None,\n",
    "#     'robot0_gripper_qpos': None,\n",
    "#     'robot0_gripper_qvel': None,\n",
    "#     'cubeA_pos': None,\n",
    "#     'cubeA_quat': None,\n",
    "#     'cubeB_pos': None,\n",
    "#     'cubeB_quat': None,\n",
    "#     'gripper_to_cubeA': None,\n",
    "#     'gripper_to_cubeB': None,\n",
    "#     'cubeA_to_cubeB': None,\n",
    "# }\n",
    "\n",
    "obs, ep_ret, ep_len = env.reset(), 0, 0\n",
    "\n",
    "o = list(obs['robot0_proprio-state']) + list(obs['object-state'])\n",
    "\n",
    "# env.viewer.set_camera(camera_id=0)\n",
    "\n",
    "\n",
    "# Define neutral value\n",
    "neutral = np.zeros(7)\n",
    "\n",
    "# Keep track of done variable to know when to break loop\n",
    "\n",
    "# Prepare for interaction with environment\n",
    "total_steps = params[\"steps_per_epoch\"] * params[\"epochs\"]\n",
    "start_time = time.time()\n",
    "\n",
    "o = torch.tensor([o], device=device)\n",
    "\n",
    "\n",
    "start_time_rec = datetime.now()\n",
    "r_true = 0\n",
    "total_main = 0\n",
    "ep_rew_main = 0\n",
    "reward_dict={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main loop: collect experience in env and update/log each epoch\n",
    "low, high = env.action_spec\n",
    "\n",
    "for t in tqdm(range(total_steps)):\n",
    "    \n",
    "    # Until start_steps have elapsed, randomly sample actions\n",
    "    # from a uniform distribution for better exploration. Afterwards, \n",
    "    # use the learned policy (with some noise, via act_noise). \n",
    "    if t > params[\"start_steps\"]:\n",
    "        a = get_action(o, params[\"act_noise\"])      # Tensor\n",
    "    else:\n",
    "        a = torch.tensor([np.random.uniform(low, high)], dtype=torch.float32, device=device)\n",
    "        \n",
    "    a_cpu = a.cpu().data.numpy()\n",
    "    # Step the env\n",
    "    obs2, r, d, _ = env.step(a_cpu[0])\n",
    "    \n",
    "    o2 = list(obs2['robot0_proprio-state']) + list(obs2['object-state'])\n",
    "    # print('len(o2) = ', len(o2))\n",
    "\n",
    "    ep_len += 1\n",
    "    total_main += r\n",
    "\n",
    "\n",
    "    # Ignore the \"done\" signal if it comes from hitting the time\n",
    "    # horizon (that is, when it's an artificial terminal signal\n",
    "    # that isn't based on the agent's state)\n",
    "    d = False if ep_len==params[\"max_ep_len\"] else d\n",
    "\n",
    "    o2 = torch.tensor([o2], dtype=torch.float32, device=device)\n",
    "    r = torch.tensor([r], dtype=torch.float32, device=device)\n",
    "    d = torch.tensor([d], dtype=torch.float32, device=device)\n",
    "\n",
    "    # Store experience to replay buffer\n",
    "    memory.push(o, a, r, o2, d)\n",
    "    nep_log[\"train/o\"].log(o)\n",
    "    nep_log[\"train/a\"].log(a)\n",
    "    nep_log[\"train/r\"].log(r)\n",
    "    nep_log[\"train/o2\"].log(o2)\n",
    "    nep_log[\"train/d\"].log(d)\n",
    "\n",
    "    # Super critical, easy to overlook step: make sure to update \n",
    "    # most recent observation!\n",
    "    o=o2\n",
    "    ep_ret += r\n",
    "    \n",
    "    \n",
    "    # End of trajectory handling\n",
    "    if d or (ep_len == params[\"max_ep_len\"]):\n",
    "        ep_rew = ep_ret/ep_len\n",
    "        obs, ep_ret, ep_len = env.reset(), 0, 0\n",
    "        o = list(obs['robot0_proprio-state']) + list(obs['object-state'])\n",
    "        o = torch.tensor([o], device=device)\n",
    "\n",
    "\n",
    "    # Update handling\n",
    "    if t >= params[\"update_after\"] and t % params[\"update_every\"] == 0:\n",
    "        for i in range(params[\"update_every\"]):\n",
    "\n",
    "            transitions = memory.sample(params[\"batch_size\"])\n",
    "            # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "            # detailed explanation). This converts batch-array of Transitions\n",
    "            # to Transition of batch-arrays.\n",
    "            batch = Transition(*zip(*transitions))\n",
    "            update(data=batch)\n",
    "\n",
    "    # End of epoch handling\n",
    "    if (t+1) % params[\"steps_per_epoch\"] == 0:\n",
    "        epoch = (t+1) // params[\"steps_per_epoch\"]\n",
    "        ep_rew_main = ep_rew\n",
    "\n",
    "        nep_log[\"train/reward\"].log(ep_rew_main)\n",
    "        print('ep_rew_main = ', ep_rew_main.cpu().data.numpy())\n",
    "        # Test the performance of the deterministic version of the agent.\n",
    "        test_agent(epoch)\n",
    "        \n",
    "\n",
    "    if (t+1) % params[\"steps_video\"] == 0:\n",
    "        epoch = (t+1) // params[\"steps_per_epoch\"]\n",
    "        now = datetime.now()\n",
    "        current_time = str(now.isoformat())\n",
    "        print('current_time = ', current_time)\n",
    "        video_agent(epoch)\n",
    "        now = datetime.now()\n",
    "        current_time = str(now.isoformat())\n",
    "        print('current_time = ', current_time)\n",
    "\n",
    "    if (t+1) % params[\"save_model_len\"] == 0:\n",
    "        epoch = (t+1) // params[\"steps_per_epoch\"]\n",
    "        now = datetime.now()\n",
    "        current_time = str(now.isoformat())\n",
    "        torch.save({\n",
    "                    'model of ac.q': ac.q.state_dict(),\n",
    "                    'model of ac.pi': ac.pi.state_dict(),\n",
    "                    'q_optimizer_state_dict': q_optimizer.state_dict(),\n",
    "                    'pi_optimizer_state_dict': pi_optimizer.state_dict(),\n",
    "                    \n",
    "                    }, \"model_nn/model_nn_%s%d.pt\" % (current_time, epoch))\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model_q's state_dict:\n",
      "q.0.weight \t torch.Size([256, 41])\n",
      "q.0.bias \t torch.Size([256])\n",
      "q.2.weight \t torch.Size([256, 256])\n",
      "q.2.bias \t torch.Size([256])\n",
      "q.4.weight \t torch.Size([256, 256])\n",
      "q.4.bias \t torch.Size([256])\n",
      "q.6.weight \t torch.Size([1, 256])\n",
      "q.6.bias \t torch.Size([1])\n",
      "Model_pi's state_dict:\n",
      "pi.0.weight \t torch.Size([256, 35])\n",
      "pi.0.bias \t torch.Size([256])\n",
      "pi.2.weight \t torch.Size([256, 256])\n",
      "pi.2.bias \t torch.Size([256])\n",
      "pi.4.weight \t torch.Size([256, 256])\n",
      "pi.4.bias \t torch.Size([256])\n",
      "pi.6.weight \t torch.Size([6, 256])\n",
      "pi.6.bias \t torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "model = ac.q\n",
    "print(\"Model_q's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "model = ac.pi\n",
    "print(\"Model_pi's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: string series 'monitoring/stdout' value was longer than 1000 characters and was truncated. This warning is printed only once per series.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pi_optimizer's state_dict:\n",
      "state \t {0: {'step': 295000, 'square_avg': tensor([[1.5294e-07, 4.3574e-07, 1.7078e-07,  ..., 4.6295e-08, 1.3153e-07,\n",
      "         3.2821e-08],\n",
      "        [1.1260e-06, 1.0042e-06, 9.5667e-07,  ..., 3.2371e-07, 2.5242e-06,\n",
      "         2.0768e-07],\n",
      "        [3.3766e-07, 1.8660e-07, 2.4575e-07,  ..., 6.6838e-08, 3.6149e-07,\n",
      "         3.6968e-08],\n",
      "        ...,\n",
      "        [3.4772e-07, 6.0496e-07, 3.3804e-07,  ..., 8.6878e-08, 3.6086e-07,\n",
      "         6.7247e-08],\n",
      "        [1.1408e-06, 7.1713e-07, 6.1176e-07,  ..., 3.4519e-07, 1.2215e-06,\n",
      "         1.1788e-07],\n",
      "        [2.8419e-07, 7.1120e-07, 3.0715e-07,  ..., 6.3061e-08, 4.3581e-07,\n",
      "         7.5016e-08]], device='cuda:0')}, 1: {'step': 295000, 'square_avg': tensor([5.5312e-07, 3.3224e-06, 6.3727e-07, 5.1333e-07, 8.2843e-07, 6.8615e-07,\n",
      "        8.8244e-07, 1.9534e-06, 6.2446e-07, 1.3387e-06, 4.0938e-07, 1.2642e-06,\n",
      "        6.8603e-07, 9.9440e-07, 1.2369e-06, 7.1836e-07, 1.1587e-06, 1.1835e-06,\n",
      "        9.5166e-07, 5.0816e-07, 1.7202e-06, 5.6938e-07, 4.4152e-07, 1.0684e-06,\n",
      "        3.1871e-07, 1.2115e-06, 7.0910e-07, 5.9612e-07, 1.3182e-06, 4.3405e-07,\n",
      "        1.3060e-06, 4.9002e-07, 1.5110e-06, 7.4731e-07, 1.3223e-06, 1.3257e-06,\n",
      "        9.5778e-07, 5.1564e-07, 1.3252e-06, 5.1387e-07, 2.5570e-06, 1.0065e-06,\n",
      "        9.3963e-07, 5.5101e-07, 1.0983e-06, 5.5319e-07, 7.3117e-07, 1.0999e-06,\n",
      "        1.3151e-06, 4.2179e-07, 5.4948e-07, 5.7533e-07, 5.4058e-07, 4.1292e-06,\n",
      "        8.9550e-07, 8.9379e-07, 1.4890e-06, 8.2242e-07, 1.0184e-06, 9.3332e-07,\n",
      "        5.0147e-07, 1.0326e-06, 5.6830e-07, 9.6716e-07, 1.6930e-06, 1.4243e-06,\n",
      "        7.0928e-07, 2.3552e-07, 9.1991e-07, 2.0595e-06, 8.3736e-07, 1.8500e-06,\n",
      "        9.6629e-07, 9.1615e-07, 6.0231e-07, 6.0253e-07, 9.5487e-07, 5.6852e-07,\n",
      "        1.6028e-06, 7.8441e-07, 2.1564e-06, 7.0696e-07, 5.4571e-07, 6.9029e-07,\n",
      "        7.2878e-07, 4.5841e-07, 1.6332e-06, 7.2337e-07, 6.3352e-07, 1.6297e-06,\n",
      "        6.0255e-07, 1.2341e-06, 1.3196e-06, 7.3362e-07, 2.3132e-06, 3.7870e-07,\n",
      "        5.7771e-07, 6.5394e-07, 1.3188e-06, 2.2837e-06, 7.8352e-07, 1.3910e-06,\n",
      "        8.8812e-07, 1.0316e-06, 4.5920e-07, 8.7999e-07, 6.6733e-07, 1.0406e-06,\n",
      "        9.5847e-07, 8.0143e-07, 4.2200e-07, 1.0488e-06, 1.0378e-06, 6.3690e-07,\n",
      "        6.9079e-07, 5.8200e-07, 2.2845e-06, 1.3906e-06, 2.7831e-06, 3.1157e-06,\n",
      "        9.7969e-07, 1.1447e-06, 1.2942e-06, 9.7422e-07, 1.2397e-06, 1.8610e-07,\n",
      "        3.3845e-07, 1.8373e-06, 1.8163e-06, 7.8021e-07, 8.3872e-07, 1.4292e-06,\n",
      "        8.2664e-07, 1.4749e-06, 4.6770e-07, 3.3704e-07, 1.9078e-06, 9.4330e-07,\n",
      "        8.8852e-07, 8.6381e-07, 1.3557e-06, 9.6310e-07, 6.6882e-07, 8.1204e-07,\n",
      "        1.5272e-06, 7.9280e-07, 1.4181e-06, 1.9098e-06, 6.6300e-07, 1.2946e-06,\n",
      "        6.9522e-07, 8.5799e-07, 1.3232e-06, 1.5567e-06, 5.0456e-07, 5.9942e-07,\n",
      "        1.0643e-06, 9.0908e-07, 6.7114e-07, 6.8561e-07, 9.7002e-07, 2.7347e-06,\n",
      "        9.8231e-07, 5.3625e-07, 1.0869e-06, 8.6739e-07, 7.0934e-07, 8.1913e-07,\n",
      "        1.3662e-06, 1.0837e-06, 2.8735e-06, 1.0980e-06, 4.4656e-07, 3.7085e-07,\n",
      "        2.9545e-07, 8.3675e-07, 1.1475e-06, 3.0978e-06, 1.2737e-06, 1.9716e-06,\n",
      "        7.4679e-07, 1.2701e-06, 7.6067e-07, 2.2446e-06, 5.6750e-07, 7.6226e-07,\n",
      "        3.2957e-07, 2.8627e-06, 9.2688e-07, 1.2293e-06, 1.1479e-06, 5.9642e-07,\n",
      "        5.2939e-07, 3.2521e-06, 1.8728e-07, 8.2832e-07, 9.6715e-07, 8.0092e-07,\n",
      "        2.3647e-06, 4.7690e-07, 3.9721e-07, 1.1170e-06, 2.1490e-06, 8.3836e-07,\n",
      "        5.1579e-07, 2.5706e-06, 8.9556e-07, 1.4429e-06, 8.6339e-07, 3.4810e-07,\n",
      "        4.8095e-06, 5.9977e-07, 6.3561e-07, 1.3872e-06, 1.5696e-06, 5.6733e-07,\n",
      "        3.9694e-07, 8.0231e-07, 1.9316e-06, 1.2127e-06, 1.7018e-06, 1.2005e-06,\n",
      "        8.4203e-07, 2.7374e-07, 1.7970e-06, 1.7229e-06, 2.9321e-06, 7.1214e-07,\n",
      "        2.5184e-07, 1.1740e-06, 1.5586e-06, 1.4483e-06, 1.1358e-06, 1.1357e-06,\n",
      "        2.2400e-07, 5.0166e-07, 5.3679e-07, 6.6559e-07, 6.8442e-07, 9.3597e-07,\n",
      "        6.9036e-07, 7.6608e-07, 8.0037e-07, 9.6158e-07, 4.2180e-07, 1.1313e-06,\n",
      "        1.5995e-06, 7.2820e-07, 1.3905e-06, 7.3579e-07, 1.3210e-06, 1.9925e-06,\n",
      "        5.4515e-07, 9.3228e-07, 2.1240e-06, 8.1638e-07], device='cuda:0')}, 2: {'step': 295000, 'square_avg': tensor([[2.6364e-07, 2.2522e-07, 2.6878e-07,  ..., 4.7396e-07, 1.0001e-07,\n",
      "         7.6052e-08],\n",
      "        [8.5503e-09, 1.2509e-07, 4.3237e-08,  ..., 1.0311e-07, 1.2524e-08,\n",
      "         1.2141e-08],\n",
      "        [6.7685e-08, 1.2867e-07, 9.7317e-08,  ..., 2.3960e-07, 3.1709e-08,\n",
      "         3.0172e-09],\n",
      "        ...,\n",
      "        [1.9853e-09, 7.1651e-08, 7.0533e-08,  ..., 5.1073e-08, 4.5405e-08,\n",
      "         2.3598e-09],\n",
      "        [4.0061e-08, 1.8999e-08, 1.1788e-08,  ..., 4.2179e-08, 8.2509e-09,\n",
      "         8.3547e-09],\n",
      "        [8.7009e-09, 1.4293e-07, 1.7964e-07,  ..., 1.2611e-07, 7.5952e-08,\n",
      "         6.9463e-11]], device='cuda:0')}, 3: {'step': 295000, 'square_avg': tensor([1.3338e-06, 2.8732e-07, 5.1800e-07, 2.6833e-07, 3.5114e-07, 2.5751e-07,\n",
      "        7.3793e-07, 1.0731e-06, 7.0303e-07, 5.2857e-07, 6.3745e-07, 9.3030e-07,\n",
      "        3.7616e-08, 2.9885e-07, 3.3626e-07, 3.2385e-07, 2.4328e-07, 2.0632e-07,\n",
      "        7.0224e-07, 6.5806e-07, 5.5312e-07, 2.7065e-07, 5.9326e-07, 1.0039e-06,\n",
      "        2.7501e-07, 4.7491e-07, 4.1329e-07, 5.3936e-07, 7.1273e-07, 7.7757e-07,\n",
      "        2.7682e-07, 4.3035e-07, 6.6462e-07, 1.7276e-07, 1.2666e-06, 6.8352e-07,\n",
      "        2.8992e-07, 5.4134e-07, 4.4876e-07, 7.2785e-07, 2.1251e-07, 4.8728e-07,\n",
      "        4.7146e-07, 1.8229e-07, 7.7180e-07, 3.1533e-07, 5.1669e-07, 8.6744e-07,\n",
      "        3.8321e-07, 5.3658e-07, 2.0011e-07, 1.8571e-06, 2.6245e-07, 2.7908e-07,\n",
      "        6.6007e-07, 4.8321e-07, 8.9602e-07, 4.6330e-07, 2.1250e-07, 8.6825e-08,\n",
      "        2.7448e-07, 2.8190e-07, 1.3591e-07, 9.7716e-08, 6.4455e-07, 2.8552e-07,\n",
      "        3.3592e-07, 2.2875e-07, 1.0917e-06, 4.5126e-07, 7.4150e-07, 5.8700e-07,\n",
      "        1.0078e-06, 3.4984e-07, 2.3437e-07, 4.3321e-07, 3.8935e-07, 3.2052e-07,\n",
      "        6.1166e-07, 4.0685e-07, 3.2441e-07, 2.1697e-07, 4.1636e-07, 3.0527e-07,\n",
      "        4.4015e-07, 7.9645e-07, 2.4019e-07, 1.8029e-07, 5.0087e-07, 6.5011e-07,\n",
      "        3.6448e-07, 4.8143e-07, 3.7006e-07, 2.1088e-06, 9.4752e-07, 7.1240e-07,\n",
      "        9.1845e-07, 8.9059e-07, 5.2570e-07, 4.3511e-07, 4.6036e-07, 1.1099e-06,\n",
      "        1.0365e-06, 3.3845e-07, 5.8313e-07, 5.4943e-07, 3.7440e-07, 4.7996e-07,\n",
      "        3.4762e-07, 5.3849e-07, 3.2128e-07, 2.9810e-07, 5.7996e-07, 8.6317e-07,\n",
      "        2.7703e-07, 4.9465e-07, 1.3240e-06, 2.2212e-07, 3.0681e-07, 1.1604e-07,\n",
      "        2.2839e-07, 3.2602e-07, 3.4324e-07, 2.4034e-07, 1.2205e-07, 3.7709e-07,\n",
      "        3.3391e-07, 6.7559e-07, 2.8020e-07, 2.2182e-07, 5.8942e-07, 1.9866e-07,\n",
      "        6.7924e-07, 1.2486e-06, 4.4893e-07, 8.2482e-07, 1.5853e-07, 6.6744e-07,\n",
      "        8.5739e-08, 4.1593e-07, 3.7109e-07, 4.2747e-07, 4.5249e-07, 1.8811e-07,\n",
      "        9.1907e-07, 2.9936e-07, 2.7776e-07, 6.0889e-07, 2.6149e-07, 8.9493e-07,\n",
      "        8.0736e-07, 1.7997e-07, 2.3811e-07, 1.1472e-06, 4.4953e-07, 1.2218e-07,\n",
      "        3.1180e-07, 3.7743e-07, 2.7124e-07, 6.0669e-07, 2.0176e-07, 2.5950e-07,\n",
      "        3.3980e-07, 4.7618e-07, 8.4249e-07, 7.9413e-07, 6.0415e-07, 6.7151e-07,\n",
      "        5.9305e-07, 5.4607e-07, 6.7823e-07, 2.2616e-07, 5.8593e-07, 5.0708e-07,\n",
      "        1.7715e-07, 6.0535e-07, 5.8566e-07, 2.2872e-07, 1.8342e-07, 5.6274e-07,\n",
      "        5.7370e-07, 4.7224e-07, 3.6537e-07, 1.0944e-06, 5.1585e-07, 3.4805e-07,\n",
      "        2.0934e-07, 1.2736e-06, 5.7299e-07, 2.5321e-07, 6.3862e-07, 7.6575e-07,\n",
      "        1.9565e-07, 1.4500e-07, 6.6701e-07, 7.0296e-07, 1.6571e-07, 7.7348e-07,\n",
      "        1.0200e-06, 2.7622e-07, 6.1201e-07, 3.1422e-07, 1.1798e-07, 4.0956e-07,\n",
      "        3.0563e-07, 8.9075e-07, 2.1461e-07, 7.2730e-07, 3.4742e-07, 1.7042e-07,\n",
      "        6.5309e-07, 9.1955e-07, 1.8736e-07, 3.1510e-07, 1.4743e-06, 4.2827e-07,\n",
      "        5.5637e-07, 4.3796e-07, 3.2133e-07, 2.8234e-07, 4.5314e-07, 1.1563e-07,\n",
      "        1.1016e-06, 1.2050e-06, 3.0249e-07, 6.0765e-07, 4.4911e-07, 1.6574e-07,\n",
      "        1.7728e-07, 1.6654e-06, 1.1465e-07, 2.3199e-07, 8.1542e-07, 7.2397e-07,\n",
      "        9.1439e-07, 3.4069e-07, 1.8869e-07, 3.8633e-07, 2.8953e-07, 6.8189e-07,\n",
      "        4.6632e-07, 1.0782e-07, 3.8895e-07, 5.6133e-07, 4.2811e-07, 1.5588e-07,\n",
      "        4.8586e-07, 7.4463e-07, 6.9351e-07, 3.1670e-07, 6.4388e-07, 5.8201e-07,\n",
      "        5.9323e-07, 2.2376e-07, 1.8152e-07, 3.8706e-07], device='cuda:0')}, 4: {'step': 295000, 'square_avg': tensor([[3.1394e-09, 9.5149e-10, 1.0238e-09,  ..., 9.0438e-11, 6.1631e-09,\n",
      "         5.9294e-11],\n",
      "        [2.2682e-09, 4.4827e-10, 6.2714e-10,  ..., 1.6272e-10, 4.1773e-09,\n",
      "         2.1893e-10],\n",
      "        [6.4304e-09, 1.2936e-09, 2.8372e-09,  ..., 7.6575e-10, 1.5071e-08,\n",
      "         8.6409e-10],\n",
      "        ...,\n",
      "        [2.3437e-09, 7.1944e-10, 1.7054e-09,  ..., 3.7593e-10, 2.3442e-09,\n",
      "         1.8384e-10],\n",
      "        [1.9351e-09, 3.4401e-10, 6.5610e-10,  ..., 7.6308e-11, 3.9624e-09,\n",
      "         1.5092e-10],\n",
      "        [2.3511e-09, 3.9019e-10, 1.2407e-09,  ..., 1.8418e-10, 2.0405e-09,\n",
      "         1.2793e-10]], device='cuda:0')}, 5: {'step': 295000, 'square_avg': tensor([9.2065e-09, 7.0973e-09, 2.3651e-08, 9.7506e-09, 5.5370e-09, 5.8348e-09,\n",
      "        3.4424e-09, 1.5745e-08, 4.9498e-09, 6.8082e-09, 9.7877e-09, 1.4240e-08,\n",
      "        5.5357e-09, 7.3659e-09, 3.1238e-09, 4.3835e-09, 1.4412e-08, 1.4305e-08,\n",
      "        1.1191e-08, 1.2285e-08, 1.0460e-08, 8.2069e-09, 6.1616e-09, 1.1412e-08,\n",
      "        1.3441e-08, 7.0059e-09, 6.3353e-09, 1.2801e-08, 3.8395e-09, 8.6126e-09,\n",
      "        1.4371e-08, 1.4981e-08, 8.8991e-09, 9.1239e-09, 1.1283e-08, 1.2043e-08,\n",
      "        2.7040e-09, 8.8261e-09, 2.4090e-09, 1.1915e-08, 1.2796e-08, 1.4876e-08,\n",
      "        7.1222e-09, 7.9508e-09, 8.7164e-09, 1.5480e-08, 9.9099e-09, 8.4846e-09,\n",
      "        6.0553e-09, 7.3750e-09, 5.6852e-09, 7.7303e-09, 2.2906e-08, 1.6367e-08,\n",
      "        3.5251e-09, 1.1791e-08, 7.5733e-09, 3.5443e-08, 7.7152e-09, 1.1595e-08,\n",
      "        7.2742e-09, 4.1447e-09, 9.4950e-09, 1.6653e-08, 2.9730e-09, 1.5146e-08,\n",
      "        1.0986e-08, 8.0505e-09, 1.1477e-08, 1.2165e-08, 5.2469e-09, 2.0293e-08,\n",
      "        5.3898e-09, 7.6215e-09, 1.5095e-08, 5.6133e-09, 6.6316e-09, 1.5686e-08,\n",
      "        8.2381e-09, 9.0120e-09, 1.6478e-08, 1.6011e-08, 2.2966e-08, 1.7814e-08,\n",
      "        1.3873e-08, 9.2506e-09, 1.3875e-08, 6.3246e-09, 2.5015e-09, 1.9397e-08,\n",
      "        1.1993e-08, 1.4565e-08, 7.3679e-09, 6.2618e-09, 4.8038e-09, 1.9393e-08,\n",
      "        1.2679e-08, 1.0975e-08, 7.5628e-09, 1.1598e-08, 4.9220e-09, 1.1663e-08,\n",
      "        8.5824e-09, 1.4010e-08, 6.4099e-09, 1.1440e-08, 1.1121e-08, 3.5779e-09,\n",
      "        7.4233e-09, 1.0481e-08, 5.6514e-09, 1.2491e-08, 2.4878e-09, 6.5521e-09,\n",
      "        7.0606e-09, 8.2678e-09, 1.0034e-08, 9.7843e-09, 8.5420e-09, 1.2525e-08,\n",
      "        6.2454e-09, 7.7457e-09, 1.9937e-08, 8.6755e-09, 9.9217e-09, 3.9916e-09,\n",
      "        9.8247e-09, 6.1970e-09, 8.1826e-09, 7.4502e-09, 1.5098e-08, 4.0280e-09,\n",
      "        6.5410e-09, 8.8682e-09, 3.8162e-09, 4.7413e-09, 3.6692e-08, 7.4869e-09,\n",
      "        1.4816e-08, 1.4210e-08, 4.4805e-09, 4.9680e-09, 9.2692e-09, 5.4416e-09,\n",
      "        2.0447e-08, 5.0088e-09, 1.3441e-08, 6.0192e-09, 6.8987e-09, 5.2347e-09,\n",
      "        4.4433e-09, 1.7076e-08, 1.5862e-08, 5.9263e-09, 1.1715e-08, 5.5197e-09,\n",
      "        4.4242e-09, 3.1045e-08, 1.5255e-08, 7.7892e-09, 1.0493e-08, 9.1635e-09,\n",
      "        1.1453e-08, 6.6077e-09, 5.1822e-09, 1.8968e-08, 2.0914e-08, 9.6755e-09,\n",
      "        7.6440e-09, 1.1165e-08, 6.7669e-09, 4.2202e-09, 5.8813e-09, 5.2459e-09,\n",
      "        7.4019e-09, 5.4443e-09, 6.2190e-09, 2.3115e-08, 1.7096e-08, 1.5924e-08,\n",
      "        8.2722e-09, 9.5527e-09, 2.8956e-09, 2.0172e-08, 8.6545e-09, 1.0099e-08,\n",
      "        2.2530e-08, 8.0111e-09, 8.4913e-09, 3.2567e-08, 9.3316e-09, 1.2699e-08,\n",
      "        1.1258e-08, 9.3863e-09, 9.3650e-09, 9.1275e-09, 5.2864e-09, 1.9495e-08,\n",
      "        6.2983e-09, 5.7350e-09, 6.5108e-09, 4.5395e-09, 7.1603e-09, 3.7605e-09,\n",
      "        1.0630e-08, 7.6619e-09, 5.9646e-09, 7.8970e-09, 4.2678e-09, 1.7743e-08,\n",
      "        1.7611e-08, 5.8940e-09, 1.8404e-08, 1.0969e-08, 3.7260e-09, 1.2455e-08,\n",
      "        1.7226e-08, 9.2261e-09, 7.6250e-09, 7.7405e-09, 9.4202e-09, 3.8526e-09,\n",
      "        3.0899e-09, 7.1657e-09, 2.5533e-09, 1.4856e-08, 2.8082e-09, 2.8267e-09,\n",
      "        1.2820e-08, 7.5158e-09, 9.6652e-09, 5.3506e-09, 1.8341e-08, 8.7977e-09,\n",
      "        5.7462e-09, 5.5747e-09, 1.2062e-08, 6.6798e-09, 1.0871e-08, 6.1218e-09,\n",
      "        1.1647e-08, 9.7184e-09, 3.4347e-09, 8.5693e-09, 4.6036e-09, 2.1932e-08,\n",
      "        9.2513e-09, 1.4924e-08, 1.2800e-08, 5.4725e-09, 6.0429e-09, 2.0492e-09,\n",
      "        1.6344e-08, 8.6694e-09, 5.7225e-09, 5.8494e-09], device='cuda:0')}, 6: {'step': 295000, 'square_avg': tensor([[1.8143e-08, 9.6778e-08, 4.4752e-08,  ..., 1.5556e-08, 8.9718e-09,\n",
      "         3.0606e-09],\n",
      "        [1.5187e-08, 8.2496e-08, 3.5922e-08,  ..., 1.3472e-08, 7.6406e-09,\n",
      "         3.1729e-09],\n",
      "        [1.4229e-08, 9.1585e-08, 3.7812e-08,  ..., 6.9501e-09, 8.1942e-09,\n",
      "         3.5396e-09],\n",
      "        [1.5162e-08, 7.6787e-08, 3.0689e-08,  ..., 9.8035e-09, 5.7403e-09,\n",
      "         2.8772e-09],\n",
      "        [1.4446e-08, 9.0092e-08, 3.1677e-08,  ..., 1.6612e-08, 8.1558e-09,\n",
      "         2.9016e-09],\n",
      "        [1.0153e-08, 5.7720e-08, 2.8019e-08,  ..., 8.9603e-09, 5.9201e-09,\n",
      "         2.2449e-09]], device='cuda:0')}, 7: {'step': 295000, 'square_avg': tensor([1.3585e-08, 1.3872e-08, 1.5136e-08, 1.0790e-08, 1.2573e-08, 8.6486e-09],\n",
      "       device='cuda:0')}}\n",
      "param_groups \t [{'lr': 0.0001, 'momentum': 0, 'alpha': 0.99, 'eps': 1e-08, 'centered': False, 'weight_decay': 0, 'params': [0, 1, 2, 3, 4, 5, 6, 7]}]\n",
      "q_optimizer's state_dict:\n",
      "state \t {0: {'step': 295000, 'square_avg': tensor([[4.1939e-06, 2.0386e-05, 1.0340e-05,  ..., 2.6008e-05, 8.6066e-06,\n",
      "         1.0164e-05],\n",
      "        [4.2373e-06, 9.3233e-05, 1.5095e-05,  ..., 3.0826e-05, 7.8962e-06,\n",
      "         6.8220e-06],\n",
      "        [3.8908e-06, 2.8025e-05, 5.3164e-05,  ..., 2.4105e-05, 1.2283e-05,\n",
      "         3.9156e-06],\n",
      "        ...,\n",
      "        [8.0495e-06, 1.4984e-04, 4.8289e-05,  ..., 2.6330e-05, 2.7567e-05,\n",
      "         3.4536e-06],\n",
      "        [3.6349e-06, 1.2911e-04, 3.5417e-05,  ..., 6.6526e-05, 5.0442e-05,\n",
      "         7.0868e-06],\n",
      "        [3.7262e-06, 6.0754e-06, 9.5844e-06,  ..., 2.8285e-06, 5.4790e-06,\n",
      "         4.2079e-06]], device='cuda:0')}, 1: {'step': 295000, 'square_avg': tensor([8.7849e-05, 9.8852e-05, 4.2656e-05, 9.5353e-06, 4.5463e-05, 1.2520e-04,\n",
      "        4.5016e-04, 4.6749e-05, 1.2066e-03, 2.3178e-05, 1.2974e-05, 1.0481e-04,\n",
      "        1.0671e-04, 6.9184e-04, 1.7677e-04, 3.2919e-05, 2.3412e-03, 2.3987e-03,\n",
      "        4.5631e-03, 1.0609e-04, 2.5966e-04, 5.2981e-05, 3.4757e-05, 2.6116e-04,\n",
      "        2.6255e-04, 2.4795e-05, 2.2004e-05, 4.2195e-05, 1.2401e-04, 3.5645e-05,\n",
      "        4.4766e-06, 7.6052e-05, 1.5305e-04, 2.7887e-04, 1.9747e-05, 2.3619e-04,\n",
      "        3.7841e-05, 4.0247e-05, 2.1463e-04, 1.4441e-04, 2.9290e-05, 6.4643e-05,\n",
      "        7.8654e-05, 9.4276e-06, 9.6274e-05, 2.9529e-04, 8.7449e-04, 4.3755e-04,\n",
      "        1.6519e-05, 2.7221e-05, 9.9027e-04, 8.1645e-04, 1.1912e-04, 7.0045e-04,\n",
      "        2.6515e-04, 2.2641e-04, 1.8238e-03, 4.4026e-04, 5.4623e-05, 5.8092e-04,\n",
      "        8.9378e-05, 3.8272e-04, 1.3166e-04, 3.9376e-04, 7.3565e-06, 1.1857e-04,\n",
      "        6.4535e-05, 1.9083e-04, 1.3562e-04, 2.5039e-04, 1.0139e-04, 1.9717e-04,\n",
      "        5.9456e-05, 1.9565e-05, 2.8555e-04, 9.1134e-04, 4.3788e-06, 2.8440e-04,\n",
      "        8.2106e-06, 1.0087e-03, 1.2028e-04, 4.7298e-05, 9.0666e-04, 1.3180e-04,\n",
      "        9.7248e-04, 1.8795e-05, 1.8423e-05, 1.2645e-04, 4.6493e-05, 3.5828e-04,\n",
      "        4.2391e-04, 5.3139e-06, 6.3490e-06, 8.5972e-05, 3.3547e-05, 5.9684e-04,\n",
      "        1.0657e-03, 3.4164e-05, 1.7844e-04, 1.3019e-04, 1.8096e-03, 2.4706e-05,\n",
      "        1.1003e-04, 3.1748e-05, 1.1282e-04, 8.5358e-05, 1.9831e-04, 1.2385e-03,\n",
      "        1.8910e-04, 1.0442e-03, 1.3086e-05, 7.0794e-04, 7.9573e-05, 6.8655e-05,\n",
      "        3.1675e-05, 2.3522e-04, 1.5557e-04, 4.9362e-05, 1.5749e-05, 4.7135e-05,\n",
      "        8.0662e-06, 7.1670e-04, 9.4128e-06, 8.8645e-05, 3.3485e-04, 9.4143e-04,\n",
      "        1.1637e-05, 7.9334e-04, 6.0275e-05, 1.3140e-04, 1.5636e-04, 7.1497e-04,\n",
      "        8.8482e-05, 5.5664e-04, 6.1084e-05, 7.4272e-05, 5.2713e-04, 1.0872e-03,\n",
      "        1.8584e-04, 8.2091e-06, 3.9458e-04, 8.5188e-04, 2.6943e-04, 5.6437e-06,\n",
      "        7.7692e-05, 1.1348e-05, 3.0929e-04, 3.1031e-04, 1.5223e-03, 7.9694e-05,\n",
      "        1.1173e-05, 7.3066e-05, 1.7918e-05, 4.3802e-04, 1.3059e-04, 9.1980e-05,\n",
      "        1.2037e-03, 9.6213e-05, 1.5834e-04, 1.4410e-03, 4.1158e-05, 9.8695e-05,\n",
      "        8.0618e-04, 1.3268e-04, 1.5222e-05, 5.4840e-05, 4.5020e-06, 3.5136e-06,\n",
      "        4.3119e-04, 6.6766e-04, 1.3152e-03, 3.0215e-04, 1.1659e-03, 5.1760e-05,\n",
      "        4.2360e-04, 1.2852e-04, 5.4765e-05, 5.3446e-04, 1.6088e-05, 5.4929e-04,\n",
      "        9.0863e-06, 1.8639e-06, 6.2785e-04, 1.5647e-05, 9.0842e-04, 5.8446e-04,\n",
      "        6.8624e-04, 2.7205e-05, 5.5646e-05, 3.2741e-04, 1.5849e-04, 2.7655e-04,\n",
      "        1.6863e-05, 7.8610e-05, 2.3567e-05, 6.7423e-04, 1.1956e-05, 1.3928e-05,\n",
      "        3.9139e-04, 9.0665e-06, 3.2174e-03, 1.4655e-04, 2.0543e-05, 1.1991e-04,\n",
      "        2.4626e-05, 1.9660e-04, 2.0324e-05, 3.4074e-05, 3.8066e-05, 7.1634e-05,\n",
      "        7.3244e-04, 1.8949e-03, 9.6042e-04, 2.0932e-03, 1.8203e-04, 8.7447e-04,\n",
      "        8.1851e-05, 7.0393e-04, 4.0586e-04, 1.2025e-05, 3.4128e-04, 1.9528e-03,\n",
      "        1.0909e-03, 1.3874e-04, 3.9225e-05, 5.3037e-05, 2.6769e-04, 2.1728e-04,\n",
      "        1.4244e-04, 3.4850e-04, 6.5478e-04, 1.4859e-03, 2.8261e-05, 1.6445e-03,\n",
      "        7.3157e-04, 2.6311e-04, 1.3992e-04, 1.0839e-03, 1.1449e-03, 7.0498e-06,\n",
      "        7.2654e-06, 3.6625e-04, 3.8629e-04, 1.7159e-04, 7.7905e-04, 1.9329e-05,\n",
      "        2.3209e-04, 2.4025e-04, 4.1458e-04, 6.9861e-04, 8.4709e-05, 1.5632e-05,\n",
      "        1.7790e-03, 3.9337e-04, 1.3727e-04, 7.8193e-06], device='cuda:0')}, 2: {'step': 295000, 'square_avg': tensor([[4.2911e-06, 2.2538e-06, 3.6651e-06,  ..., 1.7238e-05, 1.3849e-05,\n",
      "         3.2014e-06],\n",
      "        [2.9905e-06, 6.0649e-06, 4.5594e-07,  ..., 3.0999e-06, 2.7937e-06,\n",
      "         3.9508e-07],\n",
      "        [8.1861e-06, 4.6251e-06, 1.8774e-06,  ..., 3.8681e-06, 7.7600e-06,\n",
      "         1.5324e-08],\n",
      "        ...,\n",
      "        [6.2069e-09, 6.1433e-07, 2.1986e-08,  ..., 3.4447e-08, 3.3981e-08,\n",
      "         2.4977e-07],\n",
      "        [8.5373e-07, 2.1039e-06, 2.6956e-06,  ..., 1.0856e-05, 1.2775e-05,\n",
      "         1.7584e-07],\n",
      "        [8.6048e-07, 2.0775e-07, 4.7818e-07,  ..., 1.1179e-06, 7.4598e-07,\n",
      "         2.9455e-08]], device='cuda:0')}, 3: {'step': 295000, 'square_avg': tensor([2.1050e-04, 4.4140e-05, 2.9702e-05, 1.3804e-04, 7.8093e-06, 1.4336e-04,\n",
      "        4.2507e-05, 2.3322e-05, 7.8833e-06, 1.1311e-05, 1.1371e-05, 4.7772e-05,\n",
      "        8.3590e-05, 7.7291e-06, 1.4164e-05, 1.7983e-05, 3.5207e-04, 2.1336e-05,\n",
      "        2.4911e-05, 5.6384e-03, 2.1879e-04, 2.4246e-06, 5.3054e-05, 7.0065e-44,\n",
      "        3.0995e-04, 3.3343e-05, 8.4979e-06, 7.8993e-06, 9.5633e-06, 1.9962e-06,\n",
      "        1.5385e-04, 2.7260e-04, 3.6392e-04, 5.5069e-05, 4.0679e-06, 5.9398e-03,\n",
      "        1.7296e-05, 2.9112e-05, 6.7669e-05, 2.8620e-06, 6.9599e-05, 7.8767e-06,\n",
      "        1.6694e-05, 6.9726e-05, 4.0415e-02, 6.8459e-04, 7.5177e-06, 1.9444e-06,\n",
      "        1.3193e-06, 6.8488e-14, 5.0579e-05, 6.8262e-04, 2.4130e-06, 1.2339e-05,\n",
      "        1.1598e-05, 2.4048e-05, 8.9496e-06, 1.1867e-05, 3.1335e-05, 2.6877e-05,\n",
      "        7.0065e-44, 5.0692e-04, 4.9237e-04, 1.4074e-05, 1.0599e-04, 1.9185e-06,\n",
      "        2.7275e-05, 4.4490e-05, 7.3403e-06, 2.4666e-05, 1.8924e-05, 3.5257e-05,\n",
      "        2.2568e-05, 2.5803e-05, 1.4128e-05, 8.4172e-05, 3.2093e-06, 6.2860e-06,\n",
      "        6.3383e-05, 6.1707e-06, 1.9500e-06, 1.7466e-05, 2.2518e-06, 7.0065e-44,\n",
      "        7.9788e-06, 8.5299e-05, 1.5588e-05, 3.3643e-05, 1.6715e-05, 3.9396e-03,\n",
      "        1.9045e-06, 4.5298e-05, 3.4899e-05, 1.8372e-03, 1.8606e-05, 2.5517e-06,\n",
      "        7.0065e-44, 1.5433e-05, 1.4680e-05, 5.0489e-21, 2.9802e-05, 1.4034e-04,\n",
      "        3.7861e-05, 6.7677e-05, 2.0725e-06, 3.0231e-04, 4.4453e-06, 1.2430e-04,\n",
      "        2.3964e-05, 1.9882e-06, 1.4719e-04, 3.4762e-06, 7.0065e-44, 7.7974e-06,\n",
      "        2.0814e-04, 1.2906e-05, 5.3899e-04, 1.3708e-05, 1.4238e-04, 2.7612e-05,\n",
      "        1.3659e-03, 9.2879e-05, 1.2752e-05, 8.6821e-05, 1.6390e-03, 2.0107e-05,\n",
      "        7.5400e-05, 6.8893e-06, 5.2530e-05, 1.0066e-05, 8.3078e-06, 1.7413e-04,\n",
      "        4.0604e-06, 2.2565e-06, 2.5014e-05, 1.0381e-05, 1.3724e-05, 7.1509e-06,\n",
      "        5.4671e-06, 5.1613e-06, 8.0444e-05, 3.1813e-05, 2.8136e-04, 9.4577e-06,\n",
      "        1.1015e-05, 5.9216e-04, 1.3525e-06, 1.6636e-05, 1.0322e-06, 8.1112e-06,\n",
      "        9.5727e-05, 2.1960e-04, 5.9422e-05, 1.5703e-05, 2.7502e-06, 7.4609e-06,\n",
      "        9.6002e-08, 5.7911e-05, 3.6859e-05, 2.3409e-04, 4.8148e-05, 2.0828e-04,\n",
      "        4.0199e-05, 3.1377e-05, 7.0065e-44, 3.7933e-06, 7.2860e-06, 7.0065e-44,\n",
      "        5.5934e-07, 7.9428e-04, 7.0065e-44, 1.9071e-04, 2.6322e-05, 4.6078e-05,\n",
      "        3.5326e-05, 1.2941e-04, 1.3517e-04, 2.6643e-05, 7.2913e-05, 1.2805e-02,\n",
      "        2.0220e-05, 5.6862e-05, 2.1878e-05, 1.0370e-05, 2.0768e-06, 5.3132e-05,\n",
      "        4.3760e-05, 1.9172e-04, 6.9827e-06, 1.3859e-05, 7.7525e-05, 7.0065e-44,\n",
      "        5.5698e-05, 1.4482e-04, 3.4872e-05, 7.0065e-44, 2.5199e-04, 4.0766e-05,\n",
      "        6.9252e-06, 1.4475e-05, 1.1238e-05, 4.0783e-05, 1.8324e-05, 8.9072e-06,\n",
      "        7.9642e-06, 4.3297e-06, 4.3783e-06, 5.1776e-06, 5.8525e-05, 2.9122e-03,\n",
      "        1.1237e-05, 9.0858e-05, 6.0170e-06, 4.0460e-05, 1.3509e-05, 5.2171e-06,\n",
      "        1.0339e-05, 7.7761e-06, 8.7067e-06, 6.0398e-06, 2.8039e-04, 1.4206e-05,\n",
      "        1.2952e-05, 2.8747e-03, 7.8536e-06, 8.4643e-06, 7.0065e-44, 4.0749e-04,\n",
      "        1.1134e-06, 2.1715e-05, 7.3437e-06, 7.7655e-06, 5.1490e-05, 2.9075e-06,\n",
      "        7.2841e-05, 1.2185e-05, 2.8295e-06, 2.4385e-05, 9.4356e-05, 5.8227e-06,\n",
      "        3.1033e-04, 3.4100e-04, 2.3684e-03, 1.9029e-04, 4.8082e-06, 1.3853e-04,\n",
      "        1.4233e-05, 9.2471e-06, 8.2887e-04, 5.4834e-06, 7.0065e-44, 1.5350e-05,\n",
      "        1.6397e-05, 5.8132e-06, 1.3331e-04, 3.4080e-06], device='cuda:0')}, 4: {'step': 295000, 'square_avg': tensor([[3.5696e-08, 7.1484e-07, 1.3053e-06,  ..., 3.2013e-12, 4.6882e-07,\n",
      "         2.5677e-07],\n",
      "        [8.5588e-09, 6.1730e-08, 4.6834e-07,  ..., 1.6460e-11, 2.1767e-07,\n",
      "         8.3449e-07],\n",
      "        [2.8638e-09, 3.0289e-07, 4.6224e-07,  ..., 1.1766e-08, 2.2211e-07,\n",
      "         4.2029e-06],\n",
      "        ...,\n",
      "        [4.7223e-09, 3.1774e-08, 4.6597e-07,  ..., 1.0383e-10, 6.3139e-08,\n",
      "         8.3737e-07],\n",
      "        [1.6400e-08, 1.5203e-07, 1.2358e-06,  ..., 1.5718e-10, 1.4246e-07,\n",
      "         1.4657e-06],\n",
      "        [2.4833e-08, 5.1939e-07, 2.2974e-06,  ..., 1.3195e-09, 5.1189e-07,\n",
      "         2.1911e-06]], device='cuda:0')}, 5: {'step': 295000, 'square_avg': tensor([2.2069e-05, 2.4962e-05, 9.1224e-05, 9.0689e-06, 5.0521e-07, 2.5022e-06,\n",
      "        3.1789e-05, 1.9503e-05, 2.8996e-05, 1.9334e-04, 1.8955e-06, 1.1179e-05,\n",
      "        6.7576e-05, 4.7561e-05, 4.8750e-05, 2.8893e-05, 4.4214e-05, 3.2075e-05,\n",
      "        3.0891e-05, 3.0441e-06, 7.0065e-44, 1.0251e-05, 1.2461e-04, 1.3754e-04,\n",
      "        1.5680e-04, 9.3635e-07, 3.9204e-05, 1.8534e-05, 5.2271e-05, 4.0602e-05,\n",
      "        7.0065e-44, 4.1735e-05, 7.0065e-44, 4.5641e-05, 1.3835e-09, 5.0747e-07,\n",
      "        2.0517e-06, 1.9302e-06, 2.5388e-05, 1.3312e-04, 7.0065e-44, 3.3112e-05,\n",
      "        4.4596e-05, 1.1890e-04, 1.2448e-04, 2.9606e-04, 9.5033e-05, 6.1622e-05,\n",
      "        2.2032e-04, 3.1356e-05, 2.5708e-04, 2.1222e-05, 1.2598e-05, 1.9516e-05,\n",
      "        1.6576e-06, 5.8569e-05, 1.6458e-06, 1.8600e-05, 1.3311e-04, 1.0797e-07,\n",
      "        4.9058e-08, 8.8288e-25, 7.6096e-05, 6.1734e-05, 6.2216e-05, 3.5787e-04,\n",
      "        1.1773e-04, 4.9290e-05, 1.0332e-04, 1.8670e-05, 6.2633e-05, 1.2250e-05,\n",
      "        1.3122e-04, 6.0126e-05, 7.0065e-44, 1.3599e-04, 3.4468e-05, 1.1138e-04,\n",
      "        1.1484e-04, 5.9129e-05, 7.0065e-44, 2.2831e-05, 1.2766e-04, 1.3543e-04,\n",
      "        5.0757e-05, 3.2938e-05, 4.5833e-05, 7.6395e-05, 1.6005e-05, 1.6639e-04,\n",
      "        2.5280e-04, 4.5664e-05, 1.0005e-04, 2.7951e-05, 4.3632e-05, 3.8237e-06,\n",
      "        9.7638e-05, 8.5297e-05, 1.4808e-04, 6.1966e-05, 3.1191e-05, 7.8007e-05,\n",
      "        8.9718e-05, 1.3472e-04, 7.1345e-05, 1.8375e-05, 8.7227e-05, 1.3630e-06,\n",
      "        3.7947e-05, 1.0897e-04, 1.6251e-04, 6.3320e-05, 5.7951e-06, 4.6927e-05,\n",
      "        1.9161e-05, 2.6139e-05, 1.6494e-05, 7.2812e-05, 1.1774e-05, 3.0978e-05,\n",
      "        5.3004e-05, 6.5576e-07, 2.8766e-04, 7.0065e-44, 1.1008e-04, 2.0200e-05,\n",
      "        3.3459e-05, 1.7136e-04, 7.0065e-44, 6.6992e-05, 6.9359e-07, 7.0065e-44,\n",
      "        1.1168e-04, 3.3455e-05, 2.6088e-05, 4.2951e-05, 8.3882e-05, 1.0761e-05,\n",
      "        9.2526e-05, 4.6615e-05, 1.1905e-07, 7.0065e-44, 4.0672e-05, 4.8540e-05,\n",
      "        1.9549e-05, 9.4810e-05, 4.9615e-05, 3.7928e-05, 1.8083e-04, 4.3060e-06,\n",
      "        1.4069e-05, 5.4413e-05, 2.9865e-05, 8.0720e-05, 9.8365e-05, 1.3733e-04,\n",
      "        4.2220e-05, 2.3794e-05, 1.3761e-05, 6.2096e-22, 5.9376e-05, 2.5214e-05,\n",
      "        2.1652e-05, 1.0217e-04, 2.5381e-05, 3.0545e-05, 1.2046e-04, 9.1593e-05,\n",
      "        2.3666e-05, 2.4096e-05, 5.6172e-06, 3.4883e-05, 1.3848e-04, 4.2017e-05,\n",
      "        1.0685e-04, 8.6760e-06, 1.6286e-06, 5.8707e-05, 7.1062e-05, 8.0055e-05,\n",
      "        3.7067e-05, 1.4544e-05, 1.4183e-04, 1.3931e-07, 1.3112e-05, 1.1796e-05,\n",
      "        7.0065e-44, 1.4852e-04, 1.2021e-05, 1.9298e-06, 2.7710e-06, 1.3337e-04,\n",
      "        1.6564e-04, 7.0065e-44, 1.0064e-04, 4.7048e-05, 4.2430e-05, 6.3083e-05,\n",
      "        1.0909e-05, 9.7704e-05, 1.5245e-04, 8.3572e-05, 7.0536e-05, 4.2276e-06,\n",
      "        9.1367e-06, 7.0065e-44, 7.0065e-44, 9.5439e-06, 6.1408e-05, 1.6441e-04,\n",
      "        2.9764e-05, 2.1164e-05, 1.7692e-06, 4.5740e-05, 1.1129e-04, 2.0714e-05,\n",
      "        1.3145e-05, 5.7860e-06, 5.8400e-06, 1.5521e-05, 1.4707e-04, 1.0870e-05,\n",
      "        1.1019e-04, 8.7529e-05, 1.1545e-05, 2.4934e-05, 1.5160e-05, 1.2108e-04,\n",
      "        3.8679e-05, 2.1009e-04, 1.5191e-05, 2.6395e-05, 1.0201e-05, 7.0065e-44,\n",
      "        1.2909e-05, 1.4926e-05, 3.7693e-05, 8.8540e-07, 9.2054e-06, 8.8228e-05,\n",
      "        5.0694e-05, 7.2939e-06, 3.2562e-06, 5.5580e-05, 6.5899e-08, 8.0548e-05,\n",
      "        8.1230e-05, 1.7769e-04, 6.6392e-05, 1.8917e-05, 1.4363e-04, 7.9471e-05,\n",
      "        3.3204e-04, 2.3110e-05, 4.6965e-05, 7.1254e-05], device='cuda:0')}, 6: {'step': 295000, 'square_avg': tensor([[3.9940e-04, 3.0003e-04, 5.1601e-05, 5.0418e-07, 7.1377e-05, 4.9727e-05,\n",
      "         6.9208e-04, 1.6984e-06, 8.5683e-05, 9.9927e-02, 2.3124e-07, 3.3807e-06,\n",
      "         6.0389e-02, 9.9238e-06, 4.6590e-02, 1.1713e-03, 2.4637e-02, 8.8982e-03,\n",
      "         4.6853e-05, 3.1176e-07, 7.0065e-44, 9.6787e-04, 2.1084e-05, 6.6303e-02,\n",
      "         1.0708e-01, 2.3247e-07, 2.3679e-05, 1.9134e-06, 2.7203e-04, 5.7405e-02,\n",
      "         7.0065e-44, 3.5407e-04, 7.0065e-44, 5.0626e-04, 9.8972e-10, 4.1469e-06,\n",
      "         1.0155e-07, 1.1988e-04, 1.4034e-05, 2.1205e-03, 7.0065e-44, 4.8379e-02,\n",
      "         2.4560e-04, 1.1666e-02, 8.1394e-02, 1.7219e-02, 7.9107e-02, 7.3978e-05,\n",
      "         8.1367e-02, 2.8263e-05, 1.2310e-04, 1.5568e-03, 1.1003e-06, 5.0591e-06,\n",
      "         4.7197e-07, 1.0857e-02, 1.2291e-04, 7.2607e-06, 7.5486e-04, 1.6427e-08,\n",
      "         1.5169e-08, 4.4446e-27, 7.8628e-04, 5.0333e-04, 8.8368e-04, 2.0037e-04,\n",
      "         1.0439e-01, 7.0942e-06, 7.2771e-02, 3.1256e-04, 1.7278e-04, 8.0725e-06,\n",
      "         5.1481e-05, 2.6127e-02, 7.0065e-44, 8.8201e-02, 5.6978e-02, 1.2958e-02,\n",
      "         1.4610e-04, 1.8737e-05, 7.0065e-44, 3.5328e-06, 9.3453e-02, 1.0134e-03,\n",
      "         5.0992e-04, 5.9388e-02, 1.0746e-05, 4.0584e-05, 7.1249e-06, 8.6110e-02,\n",
      "         1.5749e-02, 2.5914e-04, 2.7457e-03, 5.3474e-04, 2.7068e-04, 1.2934e-06,\n",
      "         1.4998e-03, 2.5630e-02, 1.1966e-01, 5.1044e-04, 3.9779e-06, 1.3816e-02,\n",
      "         9.1931e-04, 9.7963e-05, 1.7625e-03, 8.0542e-03, 1.7102e-05, 1.4442e-07,\n",
      "         1.0194e-05, 4.7550e-04, 7.4566e-02, 4.0373e-03, 1.3496e-04, 5.7124e-02,\n",
      "         1.6302e-06, 2.2144e-06, 3.1013e-06, 2.7866e-04, 2.4394e-06, 1.1884e-05,\n",
      "         1.1054e-03, 3.1599e-08, 8.9278e-05, 7.0065e-44, 6.3237e-02, 9.3552e-06,\n",
      "         1.1592e-02, 1.6745e-02, 7.0065e-44, 3.3558e-05, 4.2776e-08, 7.0065e-44,\n",
      "         8.7390e-04, 1.9819e-05, 2.4705e-04, 1.6947e-02, 8.3153e-03, 1.4114e-06,\n",
      "         7.9981e-02, 6.8656e-02, 8.9633e-08, 7.0065e-44, 2.6260e-04, 1.3577e-05,\n",
      "         3.0730e-06, 8.9332e-02, 5.0383e-02, 7.1243e-03, 1.0712e-01, 6.3627e-07,\n",
      "         1.9955e-05, 6.2649e-03, 7.4438e-06, 1.5451e-02, 1.1854e-02, 1.9980e-02,\n",
      "         6.0905e-06, 9.5832e-06, 8.8520e-07, 8.5501e-26, 4.2721e-04, 6.9903e-06,\n",
      "         3.4387e-06, 4.6429e-02, 5.4566e-06, 1.0166e-05, 5.7251e-03, 1.1161e-02,\n",
      "         8.5327e-03, 1.2328e-03, 5.7367e-07, 3.9891e-04, 4.6188e-05, 5.0658e-02,\n",
      "         1.5980e-02, 5.5254e-06, 2.0466e-07, 1.1321e-02, 7.0848e-04, 8.1003e-04,\n",
      "         3.2640e-04, 1.0351e-05, 1.2429e-02, 3.9440e-08, 4.5683e-06, 1.3261e-06,\n",
      "         7.0065e-44, 9.1314e-02, 1.3085e-06, 2.3812e-06, 2.2424e-07, 5.6710e-04,\n",
      "         1.1087e-01, 7.0065e-44, 9.7417e-02, 3.0816e-05, 5.3656e-03, 5.6266e-06,\n",
      "         6.2112e-06, 9.0786e-02, 1.8719e-02, 8.8474e-02, 1.1178e-02, 3.9773e-07,\n",
      "         1.7527e-06, 7.0065e-44, 7.0065e-44, 8.1970e-07, 1.2756e-02, 4.9963e-05,\n",
      "         1.1741e-03, 3.5804e-04, 2.4908e-07, 2.4739e-05, 2.3587e-04, 7.8829e-04,\n",
      "         3.3659e-06, 2.9178e-06, 1.8230e-06, 1.4980e-05, 1.7556e-03, 1.1654e-04,\n",
      "         3.2213e-05, 9.9981e-04, 3.4051e-06, 1.9039e-04, 1.6919e-02, 1.2045e-01,\n",
      "         3.4580e-04, 1.4536e-02, 4.2670e-06, 3.3796e-06, 7.6231e-07, 7.0065e-44,\n",
      "         7.0881e-06, 2.0218e-04, 6.2349e-04, 1.3577e-07, 2.0583e-06, 5.8964e-02,\n",
      "         4.6225e-03, 2.7288e-06, 1.6588e-04, 8.0801e-03, 3.8683e-08, 1.3630e-02,\n",
      "         1.0342e-01, 1.3616e-01, 7.7127e-04, 2.8757e-06, 9.3089e-02, 9.5003e-03,\n",
      "         1.4992e-02, 8.0744e-03, 9.8173e-03, 2.7656e-02]], device='cuda:0')}, 7: {'step': 295000, 'square_avg': tensor([0.0162], device='cuda:0')}}\n",
      "param_groups \t [{'lr': 0.0001, 'momentum': 0, 'alpha': 0.99, 'eps': 1e-08, 'centered': False, 'weight_decay': 0, 'params': [0, 1, 2, 3, 4, 5, 6, 7]}]\n"
     ]
    }
   ],
   "source": [
    "print(\"pi_optimizer's state_dict:\")\n",
    "for var_name in pi_optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", pi_optimizer.state_dict()[var_name])\n",
    "\n",
    "print(\"q_optimizer's state_dict:\")\n",
    "for var_name in q_optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", q_optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "current_time = str(now.isoformat())\n",
    "\n",
    "\n",
    "\n",
    "torch.save({\n",
    "            'model of ac.q': ac.q.state_dict(),\n",
    "            'model of ac.pi': ac.pi.state_dict(),\n",
    "            'q_optimizer_state_dict': q_optimizer.state_dict(),\n",
    "            'pi_optimizer_state_dict': pi_optimizer.state_dict(),\n",
    "            \n",
    "            }, \"model_nn/model_nn_%s.pt\" % current_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Waiting for the remaining 140 operations to synchronize with Neptune. Do not kill this process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 140 operations synced, thanks for waiting!\n"
     ]
    }
   ],
   "source": [
    "nep_log.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1a48a149bc7a8dee0435672efcae6f64e48d62311a35302b209b3ac517d7f9c6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('cosmic_rays_x_py38_env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
